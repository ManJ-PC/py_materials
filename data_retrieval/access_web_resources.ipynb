{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<CENTER>\n",
    "<H1 style=\"color:red\">\n",
    "Accessing Web Resources with Python\n",
    "</H1>\n",
    "</CENTER>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Reference Documents</font>\n",
    "\n",
    "* <a href=\"http://zetcode.com/python/requests/\">Python Requests Tutorial</a>\n",
    "* <a href=\"https://realpython.com/python-requests/\">Python’s Requests Library (Guide)</a>\n",
    "* <a href=\"https://www.scrapehero.com/a-beginners-guide-to-web-scraping-part-1-the-basics/\">What is web scraping</a>\n",
    "* <a href=\"https://hackernoon.com/building-a-web-scraper-from-start-to-finish-bb6b95388184\">Building a Web Scraper from start to finish</a>\n",
    "* <a href=\"https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/\">Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>What will be Covered?</font>\n",
    "+ Accessing Web Pages with Requests\n",
    "+ Web Scrapping with Json\n",
    "+ Web Scrapping with Beautiful Soup\n",
    "\n",
    "![fig_scrap](https://miro.medium.com/max/1400/1*4BnBQE9Bu-EQ-gGz25x8pg.png)\n",
    "Image Source: gurutechnolabs.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Python requests</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Requests is a simple and elegant Python HTTP (Hypertext Transfer Protocol) library. \n",
    "* It provides methods for accessing Web resources via HTTP. \n",
    "* The HTTP request returns a Response Object with all the response data (content, encoding, status, etc.).\n",
    "* Requests is a built-in Python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as reqs\n",
    "\n",
    "print(reqs.__version__)\n",
    "print(reqs.__copyright__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The json module enables you to convert between JSON and Python Objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading a Web Page**\n",
    "- We use the function `get()` to grab the content of a web page into an object.\n",
    "- We extract from the object the HTML content of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me\")\n",
    "\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the module `re` to strip all the HTML markups from the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "resp = reqs.get(\"http://www.webcode.me\")\n",
    "\n",
    "content = resp.text\n",
    "\n",
    "stripped = re.sub('<[^<]+?>', '', content)\n",
    "print(stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HTTP Request**\n",
    "- An HTTP request is a message send from the client to the browser to retrieve some information or to make some action.\n",
    "- Request's request method creates a new request. \n",
    "- We use the `request` module methods: `get()`, `post()`, or `put()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `GET` request and send it to the web site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.request(method='GET', url=\"http://www.webcode.me\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the Status of a Web Page**\n",
    "- We perform an HTTP request with the `get()` method and check for the returned status.\n",
    "- 200 is a standard response for a successful HTTP request and 404 tells that the requested resource could not be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me\")\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me/news\")\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t URL:      \", resp.url)\n",
    "print(\"\\t Encoding: \", resp.encoding)\n",
    "print(\"\\t URL:      \", resp.url)\n",
    "print(\"\\t Time:     \", resp.elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`requests` `head()` Method**\n",
    "- The `head()` method retrieves document headers. \n",
    "- The headers consist of fields, including date, server, content type, or last modification time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.head(\"http://www.webcode.me\")\n",
    "\n",
    "print(\"Server: \" + resp.headers['server'])\n",
    "print(\"Last modified: \" + resp.headers['last-modified'])\n",
    "print(\"Content type: \" + resp.headers['content-type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`requests` `get()` Method**\n",
    "- The `get()` method issues a GET request to the server. \n",
    "- The GET method requests a representation of the specified resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The the following script sends a variable with a value to the `httpbin.org` server. The variable is specified directly in the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"https://httpbin.org/get?name=Peter\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get()` method takes a params parameter where we can specify the query parameters.\n",
    "\n",
    "We send a GET request to the web site and pass the data, which is specified in the params parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'name': 'Peter', 'age': 23}\n",
    "resp = reqs.get(\"https://httpbin.org/get\", params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Methods**\n",
    "\n",
    "```python\n",
    "requests.post('https://httpbin.org/post', data={'key':'value'})\n",
    "requests.put('https://httpbin.org/put', data={'key':'value'})\n",
    "requests.delete('https://httpbin.org/delete')\n",
    "requests.patch('https://httpbin.org/patch', data={'key':'value'})\n",
    "requests.options('https://httpbin.org/get')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of `requests` Methods \n",
    "\n",
    "| Method\t| Description |\n",
    "| :--- | :--- |\n",
    "| delete(url, args)\t| Sends a DELETE request to the specified url | \n",
    "| get(url, params, args)\t| Sends a GET request to the specified url | \n",
    "| head(url, args)\t| Sends a HEAD request to the specified url | \n",
    "| patch(url, data, args)\t| Sends a PATCH request to the specified url | \n",
    "| post(url, data, json, args)\t| Sends a POST request to the specified url | \n",
    "| put(url, data, args)\t| Sends a PUT request to the specified url | \n",
    "| request(method, url, args)\t| Sends a request of the specified method to the specified url| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Web Scraping</font>\n",
    "> Web scraping is a mechanism of collecting large amounts of data from the webpage and store the data into any required format which further helps us to perform analysis on the extracted data.\n",
    "\n",
    "![fig_json](https://daveberesford.co.uk/wp-content/uploads/2019/02/data-scraping-960x594.png)\n",
    "Image Source: daveberesford.co.uk\n",
    "\n",
    "Web scraping is used to extract or “scrape” data from any web page on the Internet.\n",
    "Web scraping is performed using a “**web scraper**” (or a “bot” or a “web spider” or “web crawler”). \n",
    "A web-scraper is a program that goes to web pages, downloads the contents, extracts data out of the contents and then saves the data to a file or a database.\n",
    "\n",
    "Web scraping involves a three-step process:\n",
    "\n",
    "1. **Step 1**: Send an HTTP request to the webpage you want to scrape\n",
    "     - The server responds to the request by returning the HTML content of the target webpage.\n",
    "2. **Step 2**: Parse the HTML content\n",
    "     - A parser is needed to create a nested structure of the HTML data. \n",
    "3. **Step 3**: Pull data out of HTML\n",
    "     - We use Python packages such as Json and Beautiful Soup to pull out data and store them.\n",
    "     \n",
    "![fig_scrap](https://www.scrapehero.com/wp/wp-content/uploads/2018/01/xhow-does-a-web-scraper-work-simple-2.png.pagespeed.ic.MeNRriGmi9.webp)\n",
    "Image Source: scrapehero.com\n",
    "\n",
    "Web Scrapers crawl websites, extracts data from it, transforms to a usable structured format and load it to a file or database for subsequent use.\n",
    "\n",
    "A typical web scraper has the following components:\n",
    "\n",
    "![fig_scrap](https://www.scrapehero.com/wp/wp-content/uploads/2018/01/xComponents-of-web-scraper1.png.pagespeed.ic.uNMyC_Y5W4.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Web Scraping with JSON</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.nasa.gov/planetary/apod?api_key=DEMO_KEY\"\n",
    "page_content = reqs.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data with JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "if page_content.status_code == 200:\n",
    "   json_page = json.loads(page_content.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in json_page:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the keys and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in json_page:\n",
    "    print(x+\" -->\", json_page[x])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(json_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from skimage import io\n",
    "\n",
    "io.imshow(io.imread(json_page[\"url\"]))\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.imshow(io.imread(json_page[\"hdurl\"]))\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">If you want to download the file on your local system:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "url_name = json_page[\"url\"]\n",
    "loc_file_name = os.path.basename(url_name)\n",
    "\n",
    "urllib.request.urlretrieve(url_name, loc_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Example**\n",
    "\n",
    "We will fetch data of CityBike NYC (Bike Sharing System) from specified <a href=\"https://feeds.citibikenyc.com/stations/stations.json\">https://feeds.citibikenyc.com/stations/stations.json</a> and convert into dictionary format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get JSON string data from CityBike NYC using web requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response= reqs.get(\"https://feeds.citibikenyc.com/stations/stations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check type of json_response object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(json_response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data in `loads()` function of json library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_dict = json.loads(json_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check type of news_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(bike_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bike_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get stationBeanList key data from dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bike_dict['stationBeanList'][0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(bike_dict['stationBeanList'][0], indent=3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Python Beautiful Soup</font>\n",
    "\n",
    "- Web scraping allows you to download the HTML of a website and extract the data that you need.\n",
    "- Beautiful Soup is a Python library for scrapping data from websites.\n",
    "- Beautiful Soup creates a parse tree from parsed HTML and XML documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = reqs.get(\"http://www.webcode.me\")\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a beautiful soup object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')\n",
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Title of the page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get attribute**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beginning navigation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting specific tags**\n",
    "- HTML is made up of tags. It stores all of it’s data in them, and in the midst of all that clutter lies the data we need. \n",
    "- The `find` method searches for the first tag with the needed name.\n",
    "- The `find_all` method searches for all tags with the needed tag name and returns them as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we want to find paragraph tags `<p>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find('p'))\n",
    "print(type(mysoup.find('p')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('p'))\n",
    "print(type(mysoup.find_all('p')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('p')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, paragraph in enumerate(mysoup.find_all('p')):\n",
    "    print(\"Type: {}\".format(type(paragraph)))\n",
    "    print(\"Inner Text {}: {}\".format(i+1, paragraph.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = mysoup.find_all('body')\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type body:       \", type(body))\n",
    "print(\"Type inner body: \", type(body[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(body[0].find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('body')[0].find_all('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grab the text**\n",
    "\n",
    "- Use the method `get_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Web Scraping Rules</font>\n",
    "\n",
    "![fig_ethics](https://hackernoon.com/hn-images/0*MPt2rectMhwklT63.jpg)\n",
    "\n",
    "As reference, check: <a href=\"https://info.scrapinghub.com/web-scraping-guide/web-scraping-best-practices\">The Web Scraping Best Practices Guide</a> or watch the video <a href=\"https://www.youtube.com/watch?v=i7DEy-ZB_Lk\">Is Web Scraping Legal?</a>\n",
    "\n",
    "- Check a website’s Terms and Conditions before you scrape it.\n",
    "- Do not request data from the website too aggressively with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). One request for one webpage per second is good practice.\n",
    "- The layout of a website may change from time to time, so make sure to revisit the site and rewrite your code as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "- Get information from the website `https://astg606.github.io/py_courses/virtual_courses/`\n",
    "- Print the list of all the classes offered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "- Access the website `https://www.marketwatch.com`, and\n",
    "- Extract the current values (together with the changes and percentages) of the three major indices that appear on the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Application</font>\n",
    "\n",
    "- We want to get all book names on historic New York Time Best Sellers (Business section)\n",
    "- The purpose is to:\n",
    "     1. Help to compile my reading list in 2020\n",
    "     2. Serve as reference to use Python for simple web analytics\n",
    "- We use the Python packages: `Pandas`, `Requests` and `Baeutiful Soup`\n",
    "- We save data in `pickle` and `csv` formats.\n",
    "\n",
    "The example was taken from: <a href=\"https://towardsdatascience.com/building-my-2020-reading-list-with-a-simple-python-script-b610c7f2c223\">Building my 2020 reading list with a simple Python script</a> by Pan Wu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty Pandas dataframe\n",
    "nylist = pd.DataFrame()\n",
    "\n",
    "beg_year = 2013\n",
    "end_year = 2020\n",
    "for the_year in range(beg_year, end_year):\n",
    "    for the_month in range(1, 13):\n",
    "        cur_month = str(the_month).zfill(2) # month in two digits\n",
    "        # one need to get the URL pattern first, and then use Requests package to get the URL content\n",
    "        url = 'https://www.nytimes.com/books/best-sellers/{0}/{1}/01/business-books/'.format(the_year, cur_month)\n",
    "        page = reqs.get(url)\n",
    "        print(\" --  try: {0}, {1} -- \".format(the_year, cur_month))\n",
    "        \n",
    "        # Ensure proper result is returned\n",
    "        if page.status_code != 200:\n",
    "            print(\"      Missing data for Year {} and Month {}\".format(the_year, cur_month))\n",
    "            continue\n",
    "        \n",
    "        # one may want to use BeautifulSoup to parse the right elements out\n",
    "        soup = bso(page.text, 'html.parser')\n",
    "        \n",
    "        # the specific class names are unique for this URL and they don't change across all URLs\n",
    "        top_list = soup.findAll(\"ol\", {\"class\": \"css-12yzwg4\"})[0].findAll(\"div\", {\"class\": \"css-xe4cfy\"})\n",
    "        print(\"Year: {} - Month: {} - How many in the top list: {}\".format(the_year, the_month, len(top_list)))\n",
    "        \n",
    "        # loop through the Best Seller list in each Year-Month, and append the information into a pandas DataFrame\n",
    "        for i in range(len(top_list)):\n",
    "            book   = top_list[i].contents[0]\n",
    "            title  = book.findAll(\"h3\", {\"class\": \"css-5pe77f\"})[0].text\n",
    "            author = book.findAll(\"p\",  {\"class\": \"css-hjukut\"})[0].text\n",
    "            review = book.get(\"href\")\n",
    "            # print(\"{0}, {1}; review: {2}\".format(title, author, review))\n",
    "            one_item = pd.Series([the_year, the_month, title, author, i+1, review], \n",
    "                                 index=['year', 'month', 'title', 'author', 'rank', 'review'])\n",
    "            nylist = nylist.append(one_item, ignore_index=True, sort=False)\n",
    "\n",
    "# write out the result to a pickle file for easy analysis later.\n",
    "nylist.to_pickle(\"nylist.pkl\")\n",
    "nylist.to_csv(\"nylist.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "- Write a Python script that reads the pickle file `nylist.pkl` or the csv file `nylis.csv` and prints its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
