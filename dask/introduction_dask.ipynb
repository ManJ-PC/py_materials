{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P90R-v_Xr6Wd"
   },
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<center><h1> <font color=\"red\">Introduction to Dask</font></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Reference Document</font>\n",
    "\n",
    "- <a href=\"https://docs.dask.org/en/latest/why.html\">Why Dask?</a>\n",
    "- <a href=\"https://github.com/dask/dask-tutorial\">dask-tutorial</a>\n",
    "- <a href=\"https://www.manning.com/books/data-science-with-python-and-dask\">Data Science with Python and Dask</a>\n",
    "- <a href=\"https://www.manifold.ai/dask-and-machine-learning-preprocessing-tutorial\">Dask and Machine Learning: Preprocessing Tutorial</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=uGy5gT2vLdI&feature=youtu.be\"> Working with the Python DASK library (video)</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=t_GRK4L-bnw&feature=youtu.be\">Who uses Dask (video)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_dask](https://miro.medium.com/max/1000/1*D6mSsdWECFLn6wJne4VTjg.png)\n",
    "\n",
    "\n",
    "# <font color=\"red\"> What is Dask?</font>\n",
    "\n",
    "- A flexible library for parallel computing in Python that makes it easy to build intuitive workflows for ingesting and analyzing large, distributed datasets. \n",
    "- A native parallel analytics tool designed to integrate seamlessly with Numpy, Pandas, and Scikit-Learn. \n",
    "- An out-of-core (data is read into memory from disk on an as-needed basis) parallelization library that seamlessly integrates with existing NumPy and Pandas data structures to address the following:\n",
    "     * **The vailable dataset does not fit in memory of a single machine.**\n",
    "     * **The data processing task is time consuming and needs to be sped up.**\n",
    "- Orchestrates parallel threads or processes for us and help speed up processing times.\n",
    "\n",
    "Dask consists of several different components and APIs, which can be categorized into three layers: the scheduler, low-level APIs, and high-level APIs.\n",
    "\n",
    "- Dask provides a few high-level constructs called Dask Bags, Dask DataFrames, and Dask Arrays. They provide an easy-to-use interface to parallelize many of the typical data transformations in ML workflows. \n",
    "- Dask allows the creation of highly customized job execution graphs by using their extensive Python API (e.g., `dask.delayed`) and integration with existing data structures.\n",
    "\n",
    "\n",
    "![fig_layers](http://bicortex.com/bicortex/wp-content/post_content//2019/06/Dask_APIs_Architecture.png)\n",
    "Image Source: bicortex.com\n",
    "\n",
    "\n",
    "The diagram below describes the steps Dask takes to manipulate data.\n",
    "\n",
    "- The operation is broken down into a sequence of operations on smaller partitions of our data (without having to read the whole dataset into memory).\n",
    "- Dask reads each partition as it is needed and computes the intermediate results. \n",
    "- The intermediate results are aggregated into the final result.\n",
    "- Dask handles all of that sequencing internally for us. \n",
    "- On a single machine, Dask can use threads or processors to parallelize these operations. \n",
    "\n",
    "![fig_proc](https://www.manifold.ai/hs-fs/hubfs/Blog%20Post%20Illos/ML%20pipelines%20-%20dask%20single%20machine.jpeg?width=600&name=ML%20pipelines%20-%20dask%20single%20machine.jpeg)\n",
    "Image Source: www.manifold.ai\n",
    "\n",
    "\n",
    "**Advantages of Using Dask**\n",
    "\n",
    "- Fully implemented in Python and natively scales NumPy, Pandas, and scikit-learn.\n",
    "- Can be used effectively to work with both medium datasets on a single machine and large datasets on a cluster.\n",
    "- Can be used as a general framework for parallelizing most Python objects.\n",
    "- Has a very low configuration and maintenance overhead.\n",
    "\n",
    "\n",
    "\n",
    ">Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and Pandas but can operate in parallel on datasets that don’t fit into main memory. Dask’s high-level collections are alternatives to NumPy and Pandas for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install dask[dataframe] --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlWFzeaGvPmc"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uktcOPYqt8Cn"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Dask   version: \", dask.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Parallelize Code with `dask.delayed`</font>\n",
    "\n",
    "- A simple way to parallelize the code.\n",
    "- Allows users to delay function calls into a task graph with dependencies.\n",
    "- Systems like `dask.dataframe` are built with `dask.delayed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Example**\n",
    "\n",
    "Consider the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def increment(x):\n",
    "    time.sleep(1.0)\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    time.sleep(1.0)\n",
    "    return 2 * x\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(1.0)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = increment(1)\n",
    "y = increment(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use the `dask.delayed` decorator to parallelize the functions `increment` and `add`.\n",
    "- By decorating the functions, we record what we want to compute as tasks into graphs that will be run later on parallel hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = dask.delayed(increment)(1)\n",
    "yd = dask.delayed(increment)(2)\n",
    "zd = dask.delayed(add)(xd, yd)\n",
    "zd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we call the delayed version by passing the arguments, exactly as before, but the original function isn't actually called yet.\n",
    "- A delayed object is made, which keeps track of the function to call and the arguments to pass to it.\n",
    "- We use the `visualize` method (relies on the `graphviz` package) that provide a visual representation of the operations being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zd.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we have not physically calculated **total** yet.\n",
    "- We need to apply the `compute` method to get the answer. \n",
    "- <font color=\"red\">It is only here that the data are loaded into memory for calculations</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(zd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `delayed` in Loops**\n",
    "\n",
    "Consider the sequential code with two for-loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = increment(x)\n",
    "    z = double(y)\n",
    "    out.append(z)\n",
    "    \n",
    "total = 0\n",
    "for z in out:\n",
    "    total = add(total, z)\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parallelize the above using the `delayed` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = dask.delayed(increment)(x)\n",
    "    z = dask.delayed(double)(y)\n",
    "    out.append(z)\n",
    "    \n",
    "totald = 0\n",
    "for z in out:\n",
    "    totald = dask.delayed(add)(totald, z)\n",
    "\n",
    "totald"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the visual representation through a task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totald.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(totald)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use the `delayed` decorator to parallelize the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_odd(x):\n",
    "    return (x%2)==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "results = list()\n",
    "\n",
    "for x in data:\n",
    "    if is_odd(x):\n",
    "        y = double(x)\n",
    "    else:\n",
    "        y = increment(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = sum(results)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Palindromic Words\n",
    "\n",
    "- A palindromic word is a word which characters read the same backward as forward. \n",
    "- Some examples of palindromes are `redivider`, `deified`, `civic`, `radar`, `level`, `rotor`, `kayak`, `reviver`, `racecar`, `madam`, and `refer`.\n",
    "\n",
    "We want to find the number of palindromes from a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def is_palindrome(s):\n",
    "    return s.upper() == s.upper()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_list = ['redivider', 'deified', 'civic', 'radar', 'level',\n",
    "               'rotor', 'kayak', 'reviver', 'racecar', 'madam', 'refer',\n",
    "               'Being',  'man', 'not', 'without', 'frequent', 'consciousness',\n",
    "               'that', 'there', 'was', 'some', 'charm', 'this', 'life', 'stood',\n",
    "               'still', 'after', 'looking', 'sky', 'useful', 'instrument',\n",
    "               'regarded', 'appreciative', 'spirit', 'work', 'art',\n",
    "               'superlatively', 'beautiful', 'moment', 'seemed',\n",
    "               'impressed', 'with', 'speaking', 'loneliness', 'scene',\n",
    "               'rather', 'complete', 'abstraction', 'from', 'compass',\n",
    "               'sights', 'sounds', 'man', 'Human', 'shapes', 'interferences',\n",
    "               'troubles', 'joys', 'were', 'they', 'were', 'there',\n",
    "               'seemed', 'shaded', 'hemisphere', 'globe', 'sentient', 'being',\n",
    "               'save', 'himself']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Regular Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "palindromes_py = [is_palindrome(s) for s in string_list]\n",
    "total_py = sum(palindromes_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palindromes_da = [dask.delayed(is_palindrome)(s) for s in string_list]\n",
    "total_da = dask.delayed(sum)(palindromes_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_da.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = total_da.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Important Lesson</font>**\n",
    "\n",
    "- The `delayed` decorator adds overhead.\n",
    "- It is good not to use it when a task requires a little amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Dask Array</font>\n",
    "\n",
    "- Dask arrays coordinate many Numpy arrays, arranged into chunks within a grid. \n",
    "    - _Parallel_: Uses all of the cores on your computer\n",
    "    - _Larger-than-memory_: Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "    - _Blocked Algorithms_: Perform large computations by performing many smaller computations\n",
    "- They support a large subset of the Numpy API.\n",
    "\n",
    "![fig_array](https://miro.medium.com/max/1388/1*JfQnXJ5_R104bPyE8_XhwQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Dask Array**\n",
    "\n",
    "- Create a 20000x20000 array of random numbers, represented as many numpy arrays of size 1000x1000 (or smaller if the array cannot be divided evenly). \n",
    "- There are 100 (10x10) numpy arrays of size 1000x1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((20000, 20000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Numpy syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x.T\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **`compute()`** function if you want your result as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = z.compute()\n",
    "print(type(w), w.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Persit Data in Memory**\n",
    "\n",
    "- If you have the available RAM for your dataset then you can persist data in memory.\n",
    "- This allows future computations to be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y[0, 0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy against Dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "x = np.random.normal(10, 0.1, size=(20000, 20000)) \n",
    "y = x.mean(axis=0)[::100] \n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = da.random.normal(10, 0.1, size=(20000, 20000), chunks=(1000, 1000))\n",
    "x = x.persist()\n",
    "y = x.mean(axis=0)[::100] \n",
    "#y.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Dask DataFrames</font>\n",
    "\n",
    "- Pandas is great for tabular datasets that fit in memory. \n",
    "- Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM. \n",
    "- Dask DataFrames:\n",
    "     - Coordinate many Pandas DataFrames, partitioned along an index. \n",
    "     - Support a large subset of the Pandas API.\n",
    "- One operation on a Dask DataFrame triggers many Pandas operations on the constituent pandas DataFrames in a way that is mindful of potential parallelism and memory constraints.\n",
    "- Some of the operations that are really fast if you use Dask Dataframes:\n",
    "     - Arithmetic operations (multiplying or adding to a Series)\n",
    "     - Common aggregations (`mean`, `min`, `max`, `sum`, etc.)\n",
    "     - Calling `apply`\n",
    "     - Calling `value_counts()`, `drop_duplicates()` or `corr()`\n",
    "     - Filtering with `loc`, `isin`, and row-wise selection\n",
    "\n",
    "![fig_df](https://pythondata.com/wp-content/uploads/2016/11/Screen-Shot-2016-11-24-at-6.52.24-PM-168x300.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> NYC Flights Dataset</font>\n",
    "\n",
    "Data is specific to flights out of the three airports in the New York City area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the remote data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "print(\"\\t Downloading NYC dataset...\", end=\"\\n\", flush=True)\n",
    "\n",
    "url = \"https://storage.googleapis.com/dask-tutorial-data/nycflights.tar.gz\"\n",
    "filename, header = urllib.request.urlretrieve(url, \"nycflights.tar.gz\")\n",
    "\n",
    "print(\"\\t Done!\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `.csv` files from the tar file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(filename, mode=\"r:gz\") as flights:\n",
    "     flights.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the files at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join(\"data\", \"nycflights\", \"*.csv\"), \n",
    "                parse_dates={\"Date\": [0, 1, 2]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The representation of the dataframe object contains no data. \n",
    "- `pandas.read_csv` reads in the entire file before inferring datatypes.\n",
    "- `dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file). These inferred datatypes are then enforced when reading all partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the last few rows, we have a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is an issue with the data types of few columns.\n",
    "- The datatypes inferred in the sample are incorrect.\n",
    "- We can fix it by reading the files again and specify the appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join(\"data\", \"nycflights\", \"*.csv\"), \n",
    "                parse_dates={\"Date\": [0, 1, 2]},\n",
    "                dtype={'TailNum': str,\n",
    "                       'CRSElapsedTime': float,\n",
    "                       'Cancelled': bool})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Perform Operations as with `Pandas DataFrames`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum value of a column**:\n",
    "\n",
    "- We now want to compute the maximum of the `DepDelay` column.\n",
    "- With `Pandas`, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums.\n",
    "- `dask.dataframe` allows us to write pandas-like code that operates on large than memory datasets in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.DepDelay.max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Pandas`, we will have something like:\n",
    "\n",
    "```python\n",
    "   maxes = []\n",
    "   for fn in filenames:\n",
    "       df = pd.read_csv(fn)\n",
    "       maxes.append(df.DepDelay.max())\n",
    "\n",
    "   final_max = max(maxes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Dest == 'PIT'].compute().plot(kind='scatter', \n",
    "                                    x=\"DayOfWeek\", \n",
    "                                    y=\"DepDelay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of non-cancelled flights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[~df.Cancelled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of non-cancelled flights were taken from each airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average departure delay from each airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"DayOfWeek\").DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groub by destinations and count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Dest\").count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sharing Intermediate Results**\n",
    "\n",
    "- We sometimes do the same operation more than once. \n",
    "- For most operations, `dask.dataframe` hashes the arguments, allowing duplicate computations to be shared, and only computed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = df[~df.Cancelled]\n",
    "mean_delay = non_cancelled.DepDelay.mean()\n",
    "std_delay = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass both to a single `compute` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = da.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task graphs for both results are merged when calling dask.compute, allowing shared operations to only be done once instead of twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Example of Machine Learning with Dask</font>\n",
    "\n",
    "Grab columns from the Dask DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[[\"CRSDepTime\", \"CRSArrTime\", \"Cancelled\"]]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can query the shape (note delayed # of sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic EDA**\n",
    "\n",
    "We can get descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform searches and operations on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20, \n",
    "                       input_dim=len(df_train), \n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1,  activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Model**\n",
    "\n",
    "Generate batches of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_generator(df, fraction=0.01):\n",
    "    while True:\n",
    "          batch = df.sample(frac=fraction)\n",
    "          X = batch.iloc[:, :-1]\n",
    "          y = batch.iloc[:, -1]\n",
    "          yield (X.compute(), y.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We never run of memory while doing the training:\n",
    "\n",
    "```\n",
    "   steps_per_epoch * batch_size = number_of_rows_in_train_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=batch_data_generator(df_train),\n",
    "                    steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Schedulers</font>\n",
    "\n",
    "- After Dask generates the task graphs, it needs to execute them on parallel hardware. \n",
    "- It is the role of a task scheduler. \n",
    "- There are different task schedulers. Each will consume a task graph and compute the same result, but with different performance characteristics.\n",
    "\n",
    "![Schedulers](https://docs.dask.org/en/latest/_images/collections-schedulers.png)\n",
    "Image Source: https://docs.dask.org/\n",
    "\n",
    "To execute the task graphs there are two types of schedulers:\n",
    "* **Single machine**: Provides basic features on a local process or thread pool. It is simple and cheap to use, although it can only be used on a single machine and does not scale\n",
    "* **Distributed**: Offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = dask.delayed(increment)(x)\n",
    "    z = dask.delayed(double)(y)\n",
    "    out.append(z)\n",
    "    \n",
    "totald = 0\n",
    "for z in out:\n",
    "    totald = dask.delayed(add)(totald, z)\n",
    "\n",
    "totald"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single thread**\n",
    "\n",
    "- The single-threaded synchronous scheduler executes all computations in the local thread with no parallelism at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local threads**\n",
    "\n",
    "Uses `multiprocessing.pool.ThreadPool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all the processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some of the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='threads', num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local processes**\n",
    "\n",
    "- The multiprocessing scheduler executes computations with a local `multiprocessing.Pool`.\n",
    "- Every task and all of its dependencies are shipped to a local process, executed, and then their result is shipped back to the main process. \n",
    "- Moving data to remote processes and back can introduce performance penalties, particularly when the data being transferred between processes is large. \n",
    "- The multiprocessing scheduler is an excellent choice when workflows are relatively linear, and so does not involve significant inter-task data transfer as well as when inputs and outputs are both small, like filenames and counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "print (multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = totald.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some of the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = totald.compute(scheduler='processes', num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dask distributed scheduler: Run through a Dashboard**\n",
    "\n",
    "- The Dask distributed scheduler can either be setup on a cluster or run locally on a personal machine. \n",
    "- It is a centrally managed, distributed, dynamic task scheduler. The central dask-scheduler process coordinates the actions of several dask-worker processes spread across multiple machines and the concurrent requests of several clients.\n",
    "- The scheduler is asynchronous and event-driven, simultaneously responding to requests for computation from multiple clients and tracking the progress of multiple workers.\n",
    "- The event-driven and asynchronous nature makes it flexible to concurrently handle a variety of workloads coming from multiple users at the same time while also handling a fluid worker population with failures and additions. \n",
    "- Workers communicate amongst each other for bulk data transfer over TCP.\n",
    "- To set up `dask.distributed`, we need to create client instance by calling `Client` class from `dask.distributed`. \n",
    "- It will internally create a dask scheduler and dask workers. \n",
    "- We will get the link of the dashboard where we can analyze tasks running in parallel. \n",
    "- We can pass a number of workers (using the `n_workers` argument) and threads to use per worker process (using the `threads_per_worker` argument).\n",
    "- As soon as you create a client, Dask will automatically start using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=2, threads_per_worker=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_slow_add(x, y):\n",
    "    time.sleep(random.randrange(3,10))\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for x in data:\n",
    "    y = dask.delayed(random_slow_add)(x, 1)\n",
    "    results.append(y)\n",
    "    \n",
    "total = dask.delayed(sum)(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Things to Consider</font>**\n",
    "\n",
    "- Each Dask task has overhead (about 1 ms). If you have a lot tasks this overhead can add up. It is a good idea to give each task more than a few seconds of work.\n",
    "- To better understand how your program is performing, check the [Dask Performance Diagnostics](https://distributed.dask.org/en/latest/diagnosing-performance.html) documentation. You can also view the [video](https://docs.dask.org/en/stable/diagnostics-distributed.html) to find out how to group your work into fewer, more substantial tasks. This might mean that you call lazy operations at once instead of individually. This might also repartitioning your dataframe(s).\n",
    "- A good rule of thumb for choosing number of threads per Dask worker is to choose the square root of the number of cores per processor. In general more threads per worker are good for a program that spends most of its time in NumPy, SciPy, Numba, etc., and fewer threads per worker are better for simpler programs that spend most of their time in the Python interpreter.\n",
    "- The Dask scheduler runs on a single thread, so assigning it its own node is a waste.\n",
    "- There is no hard limit on Dask scaling. The task overhead though will eventually start to swamp your calculation depending on how long each task takes to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Example with DataFrame</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Pandas DataFrame with 100000 rows and two columns with values selected randomly between 1 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100000\n",
    "df = pd.DataFrame({'X':np.random.randint(1000, size=num_rows),\n",
    "                   'Y':np.random.randint(1000, size=num_rows)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that computes the sum of square for each column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_squares(df):\n",
    "    return df.X**2 + df.Y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time it takes to call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df['add_squares'] = df.apply(add_squares,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Parallelize using Dask `Map_Partition`</font>\n",
    "\n",
    "We construct a Dask DataFrame from pandas dataframe using `from_pandas` function and specify the number of partitions (`nparitions`) to break this dataframe into.\n",
    "\n",
    "```python\n",
    "   dd = ddf.from_pandas(df, npartitions=N)\n",
    "```\n",
    "\n",
    "`ddf` is the name you imported Dask Dataframes with, and `npartitions` is an argument telling the Dataframe how you want to partition it.\n",
    "\n",
    "Each partition will run on a different thread, and communication between them will become too costly if there are too many."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will break into 4 partitions (number of available cores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(df, npartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply `add_squares` method on each of these partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ddf['z'] = ddf.map_partitions(add_squares, \n",
    "                               meta=(None, 'int64')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(x, y):\n",
    "    return y * (x**2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df1 = df.apply(lambda row: myfunc(row.X, row.Y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "ddf = dd.from_pandas(df, npartitions=4*multiprocessing.cpu_count())\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ddfz = ddf.map_partitions(lambda data: \n",
    "                              data.apply(lambda row: myfunc(row.X, row.Y), axis=1)).compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Dask and GPUs</font>\n",
    "\n",
    "- Dask can also help to scale out large array and dataframe computations by combining the Dask Array and DataFrame collections with a GPU-accelerated array or dataframe library.\n",
    "- The RAPIDS libraries provide a GPU accelerated Pandas-like library, `cuDF`, which interoperates well and is tested against Dask DataFrame.\n",
    "- You can convert a Pandas-backed Dask DataFrame to a cuDF-backed Dask DataFrame.\n",
    "\n",
    "![rapids](https://pbs.twimg.com/media/D2CeyaYVAAAe3kM.jpg)\n",
    "Image Source: NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graphics Processing Units (GPUs) are custom designed to be very efficient at handling computer graphics and image processing.\n",
    "- Central Processing Units (CPUs) handle computations serially, meaning the logic in handled in one stream: the next task will complete when the subsequent task has finished. CPUs can execute tasks in parallel across cores. For example, most computer CPUs tend to have either two, four or six cores.\n",
    "- In comparison, GPUs have hundreds of 'cores'. This massively parallel architecture is what gives the GPU its high compute performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Sanity Check #\n",
    "\n",
    "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
    "\n",
    "Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "device_name = pynvml.nvmlDeviceGetName(handle)\n",
    "\n",
    "if device_name != b'Tesla T4':\n",
    "    raise Exception(\"\"\"\n",
    "       Unfortunately this instance does not have a T4 GPU.\n",
    "       Please make sure you've configured Colab to request a GPU instance type. \n",
    "       Sometimes Colab allocates a Tesla K80 instead of a T4. \n",
    "       Resetting the instance.\n",
    "       If you get a K80 GPU, try Runtime -> Reset all runtimes...\n",
    "  \"\"\")\n",
    "else:\n",
    "    print('Great! You got the right kind of GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Setup:\n",
    "\n",
    "1. Install most recent Miniconda release compatible with Google Colab's Python install  (3.6.7)\n",
    "2. Install RAPIDS libraries\n",
    "3. Set necessary environment variables\n",
    "4. Copy RAPIDS .so files into current working directory, a workaround for conda/colab interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intall miniconda\n",
    "!wget -c https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
    "!bash ./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
    "\n",
    "# install RAPIDS packages\n",
    "!conda install -q -y --prefix /usr/local -c conda-forge \\\n",
    "  -c rapidsai-nightly/label/cuda10.0 -c nvidia/label/cuda10.0 \\\n",
    "  cudf cuml\n",
    "\n",
    "# set environment vars\n",
    "import sys, os, shutil\n",
    "sys.path.append('/usr/local/lib/python3.6/site-packages/')\n",
    "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
    "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
    "\n",
    "# copy .so files to current working dir\n",
    "for fn in ['libcudf.so', 'librmm.so']:\n",
    "    shutil.copy('/usr/local/lib/'+fn, os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster, wait, progress\n",
    "\n",
    "\n",
    "# we have one GPU, so limit Dask's workers and threads to exactly 1\n",
    "cluster = LocalCluster(processes=False)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask_xgboost\n",
    "!pip install dask_cudf\n",
    "!pip install dask_cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask_cudf\n",
    "import dask_xgboost\n",
    "\n",
    "print('Dask Version:', dask.__version__)\n",
    "print('Dask cuDF Version:', dask_cudf.__version__)\n",
    "print('Dask XGBoost Version:', dask_xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf, cuml, xgboost\n",
    "import dask_cudf, dask_cuml, dask_xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a linearly separable dataset using Gaussian Distributions.\n",
    "# The first half of the number in Y is 0 and the other half 1.\n",
    "# Therefore I made the first half of the two features quite different from\n",
    "# the second half of the features (setting the value of the means quite \n",
    "# similar) so that make quite simple the classification between the \n",
    "# classes (the data is linearly separable).\n",
    "dataset_len = 200\n",
    "dlen = int(dataset_len/2)\n",
    "X_11 = pd.Series(np.random.normal(2,2,dlen))\n",
    "X_12 = pd.Series(np.random.normal(9,2,dlen))\n",
    "X_1 = pd.concat([X_11, X_12]).reset_index(drop=True)\n",
    "X_21 = pd.Series(np.random.normal(1,3,dlen))\n",
    "X_22 = pd.Series(np.random.normal(7,3,dlen))\n",
    "X_2 = pd.concat([X_21, X_22]).reset_index(drop=True)\n",
    "X_31 = pd.Series(np.random.normal(3,1,dlen))\n",
    "X_32 = pd.Series(np.random.normal(3,4,dlen))\n",
    "X_3 = pd.concat([X_31, X_32]).reset_index(drop=True)\n",
    "Y = pd.Series(np.repeat([0,1],dlen))\n",
    "df = pd.concat([X_1, X_2, X_3, Y], axis=1)\n",
    "df.columns = ['X1', 'X2', 'X3', 'Y']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Y'], axis = 1) #.values\n",
    "y = df['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cuDF and cuML Examples #\n",
    "\n",
    "Now you can run code! \n",
    "\n",
    "What follows are basic examples where all processing takes place on the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#[cuDF](https://github.com/rapidsai/cudf)#\n",
    "\n",
    "Load a dataset into a GPU memory resident DataFrame and perform a basic calculation.\n",
    "\n",
    "Everything from CSV parsing to calculating tip percentage and computing a grouped average is done on the GPU.\n",
    "\n",
    "_Note_: You must import nvstrings and nvcategory before cudf, else you'll get errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvstrings, nvcategory, cudf\n",
    "import io, requests\n",
    "\n",
    "# download CSV file from GitHub\n",
    "url=\"https://github.com/plotly/datasets/raw/master/tips.csv\"\n",
    "content = requests.get(url).content.decode('utf-8')\n",
    "\n",
    "# read CSV from memory\n",
    "tips_df = cudf.read_csv(io.StringIO(content))\n",
    "tips_df['tip_percentage'] = tips_df['tip']/tips_df['total_bill']*100\n",
    "\n",
    "# display average tip by dining party size\n",
    "print(tips_df.groupby('size').tip_percentage.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#[cuML](https://github.com/rapidsai/cuml)#\n",
    "\n",
    "This snippet loads a \n",
    "\n",
    "As above, all calculations are performed on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "\n",
    "# Create and populate a GPU DataFrame\n",
    "df_float = cudf.DataFrame()\n",
    "df_float['0'] = [1.0, 2.0, 5.0]\n",
    "df_float['1'] = [4.0, 2.0, 1.0]\n",
    "df_float['2'] = [4.0, 2.0, 1.0]\n",
    "\n",
    "# Setup and fit clusters\n",
    "dbscan_float = cuml.DBSCAN(eps=1.0, min_samples=1)\n",
    "dbscan_float.fit(df_float)\n",
    "\n",
    "print(dbscan_float.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adv_viz.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
