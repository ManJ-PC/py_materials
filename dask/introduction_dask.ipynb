{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P90R-v_Xr6Wd"
   },
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<center><h1> <font color=\"red\">Introduction to Dask</font></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Reference Document</font>\n",
    "\n",
    "- <a href=\"https://docs.dask.org/en/latest/why.html\">Why Dask?</a>\n",
    "- <a href=\"https://github.com/dask/dask-tutorial\">dask-tutorial</a>\n",
    "- <a href=\"https://www.manning.com/books/data-science-with-python-and-dask\">Data Science with Python and Dask</a>\n",
    "- <a href=\"https://www.manifold.ai/dask-and-machine-learning-preprocessing-tutorial\">Dask and Machine Learning: Preprocessing Tutorial</a>\n",
    "- <a href=\"https://carpentries-incubator.github.io/lesson-parallel-python/aio/index.html\">Parallel Programming in Python</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=uGy5gT2vLdI&feature=youtu.be\"> Working with the Python DASK library (video)</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=t_GRK4L-bnw&feature=youtu.be\">Who uses Dask (video)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_dask](https://miro.medium.com/max/1000/1*D6mSsdWECFLn6wJne4VTjg.png)\n",
    "\n",
    "\n",
    "# <font color=\"red\"> What is Dask?</font>\n",
    "\n",
    "- A flexible library for parallel computing in Python that makes it easy to build intuitive workflows for ingesting and analyzing large, distributed datasets. \n",
    "- A native parallel analytics tool designed to integrate seamlessly with Numpy, Pandas, and Scikit-Learn. \n",
    "- An out-of-core (data is read into memory from disk on an as-needed basis) parallelization library that seamlessly integrates with existing NumPy and Pandas data structures to address the following:\n",
    "     * **The available dataset does not fit in memory of a single machine.**\n",
    "     * **The data processing task is time consuming and needs to be sped up.**\n",
    "- Orchestrates parallel threads or processes for us and help speed up processing times.\n",
    "\n",
    "Dask consists of several different components and APIs, which can be categorized into three layers: the scheduler, low-level APIs, and high-level APIs.\n",
    "\n",
    "- Dask provides a few high-level constructs called Dask Bags, Dask DataFrames, and Dask Arrays. They provide an easy-to-use interface to parallelize many of the typical data transformations in ML workflows. \n",
    "- Dask allows the creation of highly customized job execution graphs by using their extensive Python API (e.g., `dask.delayed`) and integration with existing data structures.\n",
    "\n",
    "\n",
    "![fig_layers](http://bicortex.com/bicortex/wp-content/post_content//2019/06/Dask_APIs_Architecture.png)\n",
    "Image Source: bicortex.com\n",
    "\n",
    "\n",
    "The diagram below describes the steps Dask takes to manipulate data.\n",
    "\n",
    "- The operation is broken down into a sequence of operations on smaller partitions of our data (without having to read the whole dataset into memory).\n",
    "- Dask reads each partition as it is needed and computes the intermediate results. \n",
    "- The intermediate results are aggregated into the final result.\n",
    "- Dask handles all of that sequencing internally for us. \n",
    "- On a single machine, Dask can use threads or processors to parallelize these operations. \n",
    "\n",
    "![fig_proc](https://www.manifold.ai/hs-fs/hubfs/Blog%20Post%20Illos/ML%20pipelines%20-%20dask%20single%20machine.jpeg?width=600&name=ML%20pipelines%20-%20dask%20single%20machine.jpeg)\n",
    "Image Source: www.manifold.ai\n",
    "\n",
    "\n",
    "**Advantages of Using Dask**\n",
    "\n",
    "- Fully implemented in Python and natively scales NumPy, Pandas, and scikit-learn.\n",
    "- Can be used effectively to work with both medium datasets on a single machine and large datasets on a cluster.\n",
    "- Can be used as a general framework for parallelizing most Python objects.\n",
    "- Has a very low configuration and maintenance overhead.\n",
    "\n",
    "\n",
    "\n",
    ">Dask provides high-level Array, Bag, and DataFrame collections that mimic NumPy, lists, and Pandas but can operate in parallel on datasets that don’t fit into main memory. Dask’s high-level collections are alternatives to NumPy and Pandas for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall on Processes and Threads**\n",
    "\n",
    "- A process is an execution of a program. \n",
    "- A thread is a single execution sequence within the process.\n",
    "- A process can contain multiple threads.\n",
    "- Threads are used for small tasks, whereas processes are used for more ‘heavyweight’ tasks. \n",
    "\n",
    "| Process | Thread |\n",
    "| --- | --- |\n",
    "| Heavy weight or resouce intensive | Light weight, taking lesser resources |\n",
    "| Processes do not share memory with other processes. | Threads share the memory with other threads of the same process. |\n",
    "| Inter process communication is slow. | Inter thread communication is fast. |\n",
    "| Context switching between the processes is more expensive.  | Context switching between threads of the same process is less expensive. |\n",
    "| If one is blocked, then no other can executed until the first is unblocked. | While one is blocked and waiting, a second in the same task can run. |\n",
    "| Runs independently and is isolated from other processes | One thread can read, write or change another thread's data |\n",
    "\n",
    "![threads](https://www.educative.io/api/collection/5307417243942912/5707702298738688/page/5741031244955648/image/4642578988269568)\n",
    "Image Source: educative.io\n",
    "\n",
    "The regular Python can only run one thread at the time.\n",
    "\n",
    ">Dask offers an easy and consistent way to parallelize computations that scales from a single laptop to clusters with thousands of cores. It's based on a task scheduler that distributes Python function calls across multiple threads, processes or cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install dask[dataframe] --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlWFzeaGvPmc"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uktcOPYqt8Cn"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pandas version: \", pd.__version__)\n",
    "print(\"Dask   version: \", dask.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage\n",
    "import memory_profiler\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Parallelize Code with `dask.delayed`</font>\n",
    "\n",
    "- A simple way to parallelize the code.\n",
    "- Allows users to delay function calls into a task graph with dependencies.\n",
    "- Systems like `dask.dataframe` are built with `dask.delayed`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Example**\n",
    "\n",
    "Consider the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def increment(x):\n",
    "    time.sleep(1.0)\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    time.sleep(1.0)\n",
    "    return 2 * x\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(1.0)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x = increment(1)\n",
    "y = increment(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use the `dask.delayed` decorator to parallelize the functions `increment` and `add`.\n",
    "- By decorating the functions, we record what we want to compute as tasks into graphs that will be run later on parallel hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = dask.delayed(increment)(1)\n",
    "yd = dask.delayed(increment)(2)\n",
    "zd = dask.delayed(add)(xd, yd)\n",
    "zd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we call the delayed version by passing the arguments, exactly as before, but the original function isn't actually called yet.\n",
    "- A delayed object is made, which keeps track of the function to call and the arguments to pass to it.\n",
    "- We use the `visualize` method (relies on the `graphviz` package) that provide a visual representation of the operations being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zd.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we have not physically calculated **total** yet.\n",
    "- We need to apply the `compute` method to get the answer. \n",
    "- <font color=\"red\">It is only here that the data are loaded into memory for calculations</font>.\n",
    "- The calculations are done through using a local thread pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(zd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using `delayed` in Loops**\n",
    "\n",
    "Consider the sequential code with two for-loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = increment(x)\n",
    "    z = double(y)\n",
    "    out.append(z)\n",
    "    \n",
    "total = 0\n",
    "for z in out:\n",
    "    total = add(total, z)\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can parallelize the above using the `delayed` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = dask.delayed(increment)(x)\n",
    "    z = dask.delayed(double)(y)\n",
    "    out.append(z)\n",
    "    \n",
    "totald = 0\n",
    "for z in out:\n",
    "    totald = dask.delayed(add)(totald, z)\n",
    "\n",
    "totald"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the visual representation through a task graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totald.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask.compute(totald)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Use the `delayed` decorator to parallelize the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_odd(x):\n",
    "    return x%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "results = list()\n",
    "\n",
    "for x in data:\n",
    "    if is_odd(x):\n",
    "        y = double(x)\n",
    "    else:\n",
    "        y = increment(x)\n",
    "    results.append(y)\n",
    "\n",
    "total = sum(results)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Palindromic Words\n",
    "\n",
    "- A palindromic word is a word which characters read the same backward as forward. \n",
    "- Some examples of palindromes are `redivider`, `deified`, `civic`, `radar`, `level`, `rotor`, `kayak`, `reviver`, `racecar`, `madam`, and `refer`.\n",
    "\n",
    "We want to find the number of palindromes from a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def is_palindrome(s):\n",
    "    return s.upper() == s.upper()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_list = ['redivider', 'deified', 'civic', 'radar', 'level',\n",
    "               'rotor', 'kayak', 'reviver', 'racecar', 'madam', 'refer',\n",
    "               'Being',  'man', 'not', 'without', 'frequent', 'consciousness',\n",
    "               'that', 'there', 'was', 'some', 'charm', 'this', 'life', 'stood',\n",
    "               'still', 'after', 'looking', 'sky', 'useful', 'instrument',\n",
    "               'regarded', 'appreciative', 'spirit', 'work', 'art',\n",
    "               'superlatively', 'beautiful', 'moment', 'seemed',\n",
    "               'impressed', 'with', 'speaking', 'loneliness', 'scene',\n",
    "               'rather', 'complete', 'abstraction', 'from', 'compass',\n",
    "               'sights', 'sounds', 'man', 'Human', 'shapes', 'interferences',\n",
    "               'troubles', 'joys', 'were', 'they', 'were', 'there',\n",
    "               'seemed', 'shaded', 'hemisphere', 'globe', 'sentient', 'being',\n",
    "               'save', 'himself']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Regular Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "palindromes_py = [is_palindrome(s) for s in string_list]\n",
    "total_py = sum(palindromes_py)\n",
    "total_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palindromes_da = [dask.delayed(is_palindrome)(s) for s in string_list]\n",
    "total_da = dask.delayed(sum)(palindromes_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_da.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = total_da.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use Dask Bag, we will do the same computations faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "bag = db.from_sequence(string_list)\n",
    "bag.map(is_palindrome).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "result= sum(bag.map(is_palindrome).compute())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Important Lessons</font>**\n",
    "\n",
    "- The `delayed` decorator adds overhead.\n",
    "- It is good not to use it when a task requires a little amount of time.\n",
    "- Call `delayed` on the function not the result.\n",
    "- Break up computations into many pieces. You achieve parallelism by having many delayed calls, not by using only a single one: Dask will not look inside a function decorated with `delayed` and parallelize that code internally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Use Dask to parallelize the code below (calculations of `pi`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "def approximate_pi(num_samples):\n",
    "    num_points_circ = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Select an arbitrary point in [-1,1]x[-1,1]\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "\n",
    "        # Check if the point is inside the circle\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            num_points_circ += 1\n",
    "\n",
    "    return 4 * num_points_circ / num_samples\n",
    "\n",
    "def mean(*args):\n",
    "    return sum(args) / len(args)\n",
    "\n",
    "number_samples = 10**6\n",
    "number_experiments = 10\n",
    "\n",
    "pi_approx = mean(*[approximate_pi(number_samples) for i in range(number_experiments)])\n",
    "\n",
    "print(\"Approximation of Pi: {}\".format(pi_approx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Dask Array</font>\n",
    "\n",
    "- Dask arrays coordinate many Numpy arrays, arranged into chunks within a grid. \n",
    "    - _Parallel_: Uses all of the cores on your computer\n",
    "    - _Larger-than-memory_: Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n",
    "    - _Blocked Algorithms_: Perform large computations by performing many smaller computations\n",
    "- They support a large subset of the Numpy API.\n",
    "\n",
    "![fig_array](https://miro.medium.com/max/1388/1*JfQnXJ5_R104bPyE8_XhwQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Dask Array**\n",
    "\n",
    "- Create a 20000x20000 array of random numbers, represented as many numpy arrays of size 1000x1000 (or smaller if the array cannot be divided evenly). \n",
    "- There are 400 (20x20) numpy arrays of size 1000x1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((20000, 20000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape: \",  x.shape)\n",
    "print(\"Size:  \",  x.size)\n",
    "print(\"Chunks: \", x.chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Numpy syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x.T\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = x.mean(axis=0)\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **`compute()`** function if you want your result as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = z.compute()\n",
    "print(type(w), w.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Persit Data in Memory**\n",
    "\n",
    "- If you have the available RAM for your dataset then you can persist data in memory.\n",
    "- This allows future computations to be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y[0, 0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy against Dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_numpy():\n",
    "    x = np.random.normal(10, 0.1, size=(20000, 20000)) \n",
    "    y = x.mean(axis=0)[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "f_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dask():\n",
    "    x = da.random.normal(10, 0.1, size=(20000, 20000), \n",
    "                         chunks=(1000, 1000))\n",
    "    y = x.mean(axis=0)[::100].compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%memit\n",
    "f_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_dask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshapping the chunk size might provide a better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_dask2():\n",
    "    x = da.random.normal(10, 0.1, size=(20000, 20000), \n",
    "                         chunks=(10000, 100))\n",
    "    y = x.mean(axis=0)[::100].compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "f_dask2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dask finished faster, but used more total CPU time because Dask was able to transparently parallelize the computation because of the chunk size.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Things to Consider</font>**\n",
    "\n",
    "- If your data fits in RAM and you are not performance bound, then using NumPy might be the right choice. Dask adds another layer of complexity which may get in the way.\n",
    "- If you are just looking for speedups rather than scalability then you may want to consider using Numba for manipulating Numpy arrays.\n",
    "- How to select the chunk size?\n",
    "     - Too small: huge overheads.\n",
    "     - Poorly aligned with data: inefficient reading.\n",
    "     - Recommended to have a chuck size of at least 100 Mb.\n",
    "     - Choose a chunk size that is large in order to reduce the number of chunks that Dask has to think about (which affects overhead) but also small enough so that many of them can fit in memory at once. Dask will often have as many chunks in memory as twice the number of active threads.\n",
    "   \n",
    "\n",
    "**Avoid Oversubscribing Threads**\n",
    "     \n",
    "- By default Dask will run as many concurrent tasks as you have logical cores. \n",
    "- It assumes that each task will consume about one core.\n",
    "- Many array-computing libraries (used in Dask) are themselves multi-threaded, which can cause contention and low performance.\n",
    "- For better performance, we need to explicitly specify the use of one thread:\n",
    "\n",
    "```python\n",
    "   export OMP_NUM_THREADS=1\n",
    "   export MKL_NUM_THREADS=1\n",
    "   export OPENBLAS_NUM_THREADS=1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Memory Profiling</font>\n",
    "\n",
    "- We use the `memory_profiler` package to track memory usage.\n",
    "- It's written totally in python and monitors process which is running python code as well as line by line memory usage by code. \n",
    "- We use the `memory_usage()` and pass the parameter `interval` for the frequency of measuring the memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_with_numpy():\n",
    "    # Serial implementation\n",
    "    np.arange(10**8).sum()\n",
    "\n",
    "def sum_with_dask():\n",
    "    # Parallel implementation\n",
    "    work = da.arange(10**8).sum()\n",
    "    work.compute()\n",
    "\n",
    "memory_numpy = memory_usage(sum_with_numpy, interval=0.01)\n",
    "memory_dask = memory_usage(sum_with_dask, interval=0.01)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(memory_numpy, label='numpy')\n",
    "plt.plot(memory_dask, label='dask')\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Memory / MB')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also use Dask profiling options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import Profiler, ResourceProfiler\n",
    "work = da.arange(10**8).sum()\n",
    "with Profiler() as prof, ResourceProfiler(dt=0.001) as rprof:\n",
    "    result2 = work.compute()\n",
    "\n",
    "from bokeh.plotting import output_notebook\n",
    "from dask.diagnostics import visualize\n",
    "visualize([prof,rprof], output_notebook())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ResourceProfiler(dt=0.001) as rprof2:\n",
    "    result = np.arange(10**8).sum()\n",
    "visualize([rprof2], output_notebook())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Dask DataFrames</font>\n",
    "\n",
    "- Pandas is great for tabular datasets that fit in memory. \n",
    "- Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM. \n",
    "- Dask DataFrames:\n",
    "     - Coordinate many Pandas DataFrames, partitioned along an index. \n",
    "     - Support a large subset of the Pandas API.\n",
    "- One operation on a Dask DataFrame triggers many Pandas operations on the constituent pandas DataFrames in a way that is mindful of potential parallelism and memory constraints.\n",
    "- Some of the operations that are really fast if you use Dask Dataframes:\n",
    "     - Arithmetic operations (multiplying or adding to a Series)\n",
    "     - Common aggregations (`mean`, `min`, `max`, `sum`, etc.)\n",
    "     - Calling `apply`\n",
    "     - Calling `value_counts()`, `drop_duplicates()` or `corr()`\n",
    "     - Filtering with `loc`, `isin`, and row-wise selection\n",
    "\n",
    "![fig_df](https://pythondata.com/wp-content/uploads/2016/11/Screen-Shot-2016-11-24-at-6.52.24-PM-168x300.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> NYC Flights Dataset</font>\n",
    "\n",
    "Data is specific to flights (in 1990's) out of the three airports in the New York City area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the remote data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "print(\"\\t Downloading NYC dataset...\", end=\"\\n\", flush=True)\n",
    "\n",
    "url = \"https://storage.googleapis.com/dask-tutorial-data/nycflights.tar.gz\"\n",
    "filename, header = urllib.request.urlretrieve(url, \"nycflights.tar.gz\")\n",
    "\n",
    "print(\"\\t Done!\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the `.csv` files from the tar file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(filename, mode=\"r:gz\") as flights:\n",
    "     flights.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt data/nycflights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the files at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = dd.read_csv(os.path.join(\"data\", \"nycflights\", \"*.csv\"), \n",
    "                parse_dates={\"Date\": [0, 1, 2]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The representation of the dataframe object contains no data. \n",
    "- `pandas.read_csv` reads in the entire file before inferring datatypes.\n",
    "- `dask.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file). These inferred datatypes are then enforced when reading all partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the last few rows, we have a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is an issue with the data types of few columns.\n",
    "- The datatypes inferred in the sample are incorrect.\n",
    "- We can fix it by reading the files again and specify the appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join(\"data\", \"nycflights\", \"*.csv\"), \n",
    "                parse_dates={\"Date\": [0, 1, 2]},\n",
    "                dtype={'TailNum': str,\n",
    "                       'CRSElapsedTime': float,\n",
    "                       'Cancelled': bool})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Perform Operations as with `Pandas DataFrames`</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum value of a column**:\n",
    "\n",
    "- We now want to compute the maximum of the `DepDelay` column.\n",
    "- With `Pandas`, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums.\n",
    "- `dask.dataframe` allows us to write pandas-like code that operates on large than memory datasets in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DepDelay.max().visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.DepDelay.max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do the same thing in `Pandas`, we will have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import glob\n",
    "\n",
    "list_files = glob.glob(\"data/nycflights/*csv\")\n",
    "   \n",
    "maxes = list()\n",
    "for file_name in list_files:\n",
    "    pddf = pd.read_csv(file_name)\n",
    "    maxes.append(pddf.DepDelay.max())\n",
    "\n",
    "final_max = max(maxes)\n",
    "\n",
    "print(\"Final Maximum: \", max(maxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Dest == 'PIT'].compute().plot(kind='scatter', \n",
    "                                    x=\"DayOfWeek\", \n",
    "                                    y=\"DepDelay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of non-cancelled flights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[~df.Cancelled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of non-cancelled flights were taken from each airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average departure delay from each airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"DayOfWeek\").DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by destinations and count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Dest\").count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Dest\")[\"ArrDelay\"].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.ArrDelay+df.DepDelay>30.0].groupby(\"Dest\").Dest.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sharing Intermediate Results**\n",
    "\n",
    "- We sometimes do the same operation more than once. \n",
    "- For most operations, `dask.dataframe` hashes the arguments, allowing duplicate computations to be shared, and only computed once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = df[~df.Cancelled]\n",
    "mean_delay = non_cancelled.DepDelay.mean()\n",
    "std_delay = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass both to a single `compute` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = da.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task graphs for both results are merged when calling dask.compute, allowing shared operations to only be done once instead of twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "- Consider the code below that computes the mean departure delay per airport. \n",
    "- Parallelize the code using Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_files' is not defined"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "sum_delays = list()\n",
    "count_delays = list()\n",
    "\n",
    "for file_name in list_files:\n",
    "    pddf = pd.read_csv(file_name)\n",
    "    by_origin = pddf.groupby('Origin')\n",
    "    loc_total = by_origin.DepDelay.sum()\n",
    "    loc_count = by_origin.DepDelay.count()\n",
    "    sum_delays.append(loc_total)\n",
    "    count_delays.append(loc_count)\n",
    "\n",
    "total_delays = sum(sum_delays)\n",
    "n_flights = sum(count_delays)\n",
    "mean_delays = total_delays / n_flights\n",
    "print(\"Mean delays: {}\".format(mean_delays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Example of Machine Learning with Dask</font>\n",
    "\n",
    "Grab columns from the Dask DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[[\"CRSDepTime\", \"CRSArrTime\", \"Cancelled\"]]\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can query the shape (note delayed # of sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = len(df_train.columns)\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic EDA**\n",
    "\n",
    "We can get descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform searches and operations on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(20, \n",
    "                       input_dim=num_cols, \n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(1,  activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Model**\n",
    "\n",
    "Generate batches of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data_generator(df, fraction=0.01):\n",
    "    while True:\n",
    "          batch = df.sample(frac=fraction)\n",
    "          X = batch.iloc[:, :-1]\n",
    "          y = batch.iloc[:, -1]\n",
    "          yield (X.compute(), y.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We never run of memory while doing the training:\n",
    "\n",
    "```\n",
    "   steps_per_epoch * batch_size = number_of_rows_in_train_data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(generator=batch_data_generator(df_train),\n",
    "                    steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Schedulers</font>\n",
    "\n",
    "- After Dask generates the task graphs, it needs to execute them on parallel hardware. \n",
    "- It is the role of a task scheduler. \n",
    "- There are different task schedulers. Each will consume a task graph and compute the same result, but with different performance characteristics.\n",
    "\n",
    "![schedulers](https://docs.dask.org/en/latest/_images/dask-overview.svg)\n",
    "\n",
    "Image Source: [https://docs.dask.org/en/latest/](https://docs.dask.org/en/latest/)\n",
    "\n",
    "To execute the task graphs there are two types of schedulers:\n",
    "* **Single machine**: Provides basic features on a local process or thread pool. It is simple and cheap to use, although it can only be used on a single machine and does not scale\n",
    "* **Distributed**: Offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Single Machine Scheduler</font>\n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n = 10\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = dask.delayed(increment)(x)\n",
    "    z = dask.delayed(double)(y)\n",
    "    out.append(z)\n",
    "    \n",
    "totald = 0\n",
    "for z in out:\n",
    "    totald = dask.delayed(add)(totald, z)\n",
    "\n",
    "totald"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single thread**\n",
    "\n",
    "- The single-threaded synchronous scheduler executes all computations in the local thread with no parallelism at all.\n",
    "- It is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='synchronous')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local threads**\n",
    "\n",
    "Uses `multiprocessing.pool.ThreadPool`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all the processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='threads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some of the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time totald.compute(scheduler='threads', num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Local processes**\n",
    "\n",
    "- The multiprocessing scheduler executes computations with a local `multiprocessing.Pool`.\n",
    "- Every task and all of its dependencies are shipped to a local process, executed, and then their result is shipped back to the main process. \n",
    "- Moving data to remote processes and back can introduce performance penalties, particularly when the data being transferred between processes is large. \n",
    "- The multiprocessing scheduler is an excellent choice when workflows are relatively linear, and so does not involve significant inter-task data transfer as well as when inputs and outputs are both small, like filenames and counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "print (multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = totald.compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use some of the processors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = totald.compute(scheduler='processes', num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Distributed Scheduler</font>\n",
    "\n",
    "- The Dask distributed scheduler can either be setup on a cluster or run locally on a personal machine. \n",
    "- It is a centrally managed, distributed, dynamic task scheduler. \n",
    "     - The central dask-scheduler process coordinates the actions of several dask-worker processes spread across multiple machines and the concurrent requests of several clients.\n",
    "     - The scheduler is asynchronous and event-driven, simultaneously responding to requests for computation from multiple clients and tracking the progress of multiple workers.\n",
    "     - The event-driven and asynchronous nature makes it flexible to concurrently handle a variety of workloads coming from multiple users at the same time while also handling a fluid worker population with failures and additions. \n",
    "     - Workers communicate amongst each other for bulk data transfer over TCP.\n",
    "- To set up `dask.distributed`, we need to create client instance by calling `Client` class from `dask.distributed`. \n",
    "- It will internally create a dask scheduler and dask workers. \n",
    "- We will get the **link of the dashboard** where we can analyze tasks running in parallel. \n",
    "- We can pass a number of workers (using the `n_workers` argument) and threads to use per worker process (using the `threads_per_worker` argument).\n",
    "- As soon as you create a client, Dask will automatically start using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "#client = Client(n_workers=2, threads_per_worker=4)\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aren’t in jupyterlab and using the `dask-labextension`, you can  click the `Dashboard` link to open up the diagnostics dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_slow_add(x, y):\n",
    "    time.sleep(random.randrange(3,10))\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for x in data:\n",
    "    y = dask.delayed(random_slow_add)(x, 1)\n",
    "    results.append(y)\n",
    "    \n",
    "total = dask.delayed(sum)(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time result = total.compute()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Things to Consider</font>**\n",
    "\n",
    "- Each Dask task has overhead (about 1 ms). If you have a lot tasks this overhead can add up. It is a good idea to give each task more than a few seconds of work.\n",
    "- To better understand how your program is performing, check the [Dask Performance Diagnostics](https://distributed.dask.org/en/latest/diagnosing-performance.html) documentation. You can also view the [video](https://docs.dask.org/en/stable/diagnostics-distributed.html) to find out how to group your work into fewer, more substantial tasks. This might mean that you call lazy operations at once instead of individually. This might also repartitioning your dataframe(s).\n",
    "- A good rule of thumb for choosing number of threads per Dask worker is to choose the square root of the number of cores per node. \n",
    "     - In general more threads per worker are good for a program that spends most of its time in NumPy, SciPy, Numba, etc., and fewer threads per worker are better for simpler programs that spend most of their time in the Python interpreter.\n",
    "- The Dask scheduler runs on a single thread, so assigning it its own node is a waste.\n",
    "- There is no hard limit on Dask scaling. The task overhead though will eventually start to swamp your calculation depending on how long each task takes to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Example with DataFrame</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Pandas DataFrame with 100000 rows and two columns with values selected randomly between 1 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100000\n",
    "df = pd.DataFrame({'X':np.random.randint(1000, size=num_rows),\n",
    "                   'Y':np.random.randint(1000, size=num_rows)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that computes the sum of square for each column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_squares(df):\n",
    "    return df.X**2 + df.Y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time it takes to call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df['add_squares'] = df.apply(add_squares,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Parallelize using Dask `Map_Partition`</font>\n",
    "\n",
    "We construct a Dask DataFrame from pandas dataframe using `from_pandas` function and specify the number of partitions (`nparitions`) to break this dataframe into.\n",
    "\n",
    "```python\n",
    "   dd = ddf.from_pandas(df, npartitions=N)\n",
    "```\n",
    "\n",
    "`ddf` is the name you imported Dask Dataframes with, and `npartitions` is an argument telling the Dataframe how you want to partition it.\n",
    "\n",
    "Each partition will run on a different thread, and communication between them will become too costly if there are too many."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will break into 4 partitions (number of available cores):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(df, npartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply `add_squares` method on each of these partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf['z'] = ddf.map_partitions(add_squares, \n",
    "                               meta=(None, 'int64')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(x, y):\n",
    "    return y * (x**2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df1 = df.apply(lambda row: myfunc(row.X, row.Y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "ddf = dd.from_pandas(df, npartitions=4*multiprocessing.cpu_count())\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddfz = ddf.map_partitions(lambda data: \n",
    "                              data.apply(lambda row: myfunc(row.X, row.Y), axis=1)).compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Dask and GPUs</font>\n",
    "\n",
    "- Dask can also help to scale out large array and dataframe computations by combining the Dask Array and DataFrame collections with a GPU-accelerated array or dataframe library.\n",
    "- The RAPIDS libraries provide a GPU accelerated Pandas-like library, `cuDF`, which interoperates well and is tested against Dask DataFrame.\n",
    "- You can convert a Pandas-backed Dask DataFrame to a cuDF-backed Dask DataFrame.\n",
    "\n",
    "![rapids](https://pbs.twimg.com/media/D2CeyaYVAAAe3kM.jpg)\n",
    "Image Source: NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graphics Processing Units (GPUs) are custom designed to be very efficient at handling computer graphics and image processing.\n",
    "- Central Processing Units (CPUs) handle computations serially, meaning the logic in handled in one stream: the next task will complete when the subsequent task has finished. CPUs can execute tasks in parallel across cores. For example, most computer CPUs tend to have either two, four or six cores.\n",
    "- In comparison, GPUs have hundreds of 'cores'. This massively parallel architecture is what gives the GPU its high compute performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Sanity Check #\n",
    "\n",
    "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
    "\n",
    "Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "device_name = pynvml.nvmlDeviceGetName(handle)\n",
    "\n",
    "if device_name != b'Tesla T4':\n",
    "    raise Exception(\"\"\"\n",
    "                    Unfortunately this instance does not have a T4 GPU.\n",
    "                    Please make sure you've configured Colab to request \n",
    "                    a GPU instance type. \n",
    "                    Sometimes Colab allocates a Tesla K80 instead of a T4. \n",
    "                    Resetting the instance.\n",
    "                    If you get a K80 GPU, try Runtime -> Reset all runtimes...\n",
    "  \"\"\")\n",
    "else:\n",
    "    print('Great! You got the right kind of GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intall RAPIDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!bash rapidsai-csp-utils/colab/rapids-colab.sh stable\n",
    "\n",
    "import sys, os\n",
    "\n",
    "dist_package_index = sys.path.index('/usr/local/lib/python3.6/dist-packages')\n",
    "sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.6/site-packages'] + sys.path[dist_package_index:]\n",
    "sys.path\n",
    "exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What follows are basic examples where all processing takes place on the GPU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [cuDF](https://github.com/rapidsai/cudf)\n",
    "\n",
    "Load a dataset into a GPU memory resident DataFrame and perform a basic calculation.\n",
    "\n",
    "Everything from CSV parsing to calculating tip percentage and computing a grouped average is done on the GPU.\n",
    "\n",
    "_Note_: You must import nvstrings and nvcategory before cudf, else you'll get errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvstrings, nvcategory, cudf\n",
    "import io, requests\n",
    "\n",
    "# download CSV file from GitHub\n",
    "url=\"https://github.com/plotly/datasets/raw/master/tips.csv\"\n",
    "content = requests.get(url).content.decode('utf-8')\n",
    "\n",
    "# read CSV from memory\n",
    "tips_df = cudf.read_csv(io.StringIO(content))\n",
    "tips_df['tip_percentage'] = tips_df['tip']/tips_df['total_bill']*100\n",
    "\n",
    "# display average tip by dining party size\n",
    "print(tips_df.groupby('size').tip_percentage.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [cuML](https://github.com/rapidsai/cuml)\n",
    "\n",
    "This snippet loads a \n",
    "\n",
    "As above, all calculations are performed on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "\n",
    "# Create and populate a GPU DataFrame\n",
    "df_float = cudf.DataFrame()\n",
    "df_float['0'] = [1.0, 2.0, 5.0]\n",
    "df_float['1'] = [4.0, 2.0, 1.0]\n",
    "df_float['2'] = [4.0, 2.0, 1.0]\n",
    "\n",
    "# Setup and fit clusters\n",
    "dbscan_float = cuml.DBSCAN(eps=1.0, min_samples=1)\n",
    "dbscan_float.fit(df_float)\n",
    "\n",
    "print(dbscan_float.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [cuDF and Dask-cuDF](https://docs.rapids.ai/api/cudf/stable/10min.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask_cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "\n",
    "print('Dask cuDF Version:', dask_cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `cudf` to create a dataframe and perform operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100000\n",
    "df = cudf.DataFrame({'X':np.random.randint(1000, size=num_rows),\n",
    "                     'Y':np.random.randint(1000, size=num_rows)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_squares(df):\n",
    "    return df.X**2 + df.Y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['add_squares'] = add_squares(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same as above using `dask_cudf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask_cudf.from_cudf(df, npartitions=4)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf['z'] = add_squares(ddf).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time Series Data**\n",
    "\n",
    "`DataFrames` supports `datetime` typed columns, which allow users to interact with and filter data based on specific timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "date_df = cudf.DataFrame()\n",
    "date_df['date'] = pd.date_range('01/05/1980', periods=15000, freq='D')\n",
    "date_df['value'] = cp.random.sample(len(date_df))\n",
    "date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_date1 = dt.datetime.strptime('2001-09-11', '%Y-%m-%d')\n",
    "search_date2 = dt.datetime.strptime('2019-11-23', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "date_df.query('date >= @search_date1' and 'date <= @search_date2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ddf = dask_cudf.from_cudf(date_df, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "date_ddf.query('date >= @search_date1' and 'date <= @search_date2', \n",
    "               local_dict={'search_date1':search_date1, \n",
    "                           'search_date2':search_date2}).compute()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adv_viz.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
