{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P90R-v_Xr6Wd"
   },
   "source": [
    "<center><img src=\"http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg\"></center>\n",
    "\n",
    "<center>\n",
    "<h1><font size=\"+3\">GSFC Python Bootcamp</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center><h1> <font color=\"red\">Introduction to Dask</font></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Reference Document</font>\n",
    "\n",
    "- <a href=\"https://docs.dask.org/en/latest/why.html\">Why Dask?</a>\n",
    "- <a href=\"https://www.manning.com/books/data-science-with-python-and-dask\">Data Science with Python and Dask</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_dask](https://miro.medium.com/max/1000/1*D6mSsdWECFLn6wJne4VTjg.png)\n",
    "\n",
    "\n",
    "### What is Dask?\n",
    "\n",
    "- A flexible library for parallel computing in Python that makes it easy to build intuitive workflows for ingesting and analyzing large, distributed datasets. \n",
    "- A native parallel analytics tool designed to integrate seamlessly with Numpy, Pandas, and Scikit-Learn. \n",
    "\n",
    "Dask consists of several different components and APIs, which can be categorized into three layers: the scheduler, low-level APIs, and high-level APIs.\n",
    "\n",
    "![fig_layers](https://dpzbhybb2pdcj.cloudfront.net/daniel/HighResolutionFigures/figure_1-1.png)\n",
    "Image Source: \n",
    "\n",
    "**Advantages of Using Dask**\n",
    "\n",
    "- Fully implemented in Python and natively scales NumPy, Pandas, and scikit-learn.\n",
    "- Can be used effectively to work with both medium datasets on a single machine and large datasets on a cluster.\n",
    "- Can be used as a general framework for parallelizing most Python objects.\n",
    "- Has a very low configuration and maintenance overhead.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlWFzeaGvPmc"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uktcOPYqt8Cn"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Array\n",
    "\n",
    "- Dask arrays coordinate many Numpy arrays, arranged into chunks within a grid. \n",
    "- They support a large subset of the Numpy API.\n",
    "\n",
    "![fig_array](https://miro.medium.com/max/1388/1*JfQnXJ5_R104bPyE8_XhwQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a Dask Array**\n",
    "\n",
    "- Create a 20000x20000 array of random numbers, represented as many numpy arrays of size 1000x1000 (or smaller if the array cannot be divided evenly). \n",
    "- There are 100 (10x10) numpy arrays of size 1000x1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((20000, 20000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Numpy syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + x.T\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `compute()` function if you want your result as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = z.compute()\n",
    "print(type(w), w.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Persit Data in Memory**\n",
    "\n",
    "- If you have the available RAM for your dataset then you can persist data in memory.\n",
    "- This allows future computations to be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y[0, 0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numpy against Dask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "x = np.random.normal(10, 0.1, size=(20000, 20000)) \n",
    "y = x.mean(axis=0)[::100] \n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = da.random.normal(10, 0.1, size=(20000, 20000), chunks=(1000, 1000))\n",
    "x = x.persist()\n",
    "y = x.mean(axis=0)[::100] \n",
    "#y.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask DataFrames\n",
    "\n",
    "- Coordinate many Pandas dataframes, partitioned along an index. \n",
    "- Support a large subset of the Pandas API.\n",
    "\n",
    "![fig_df](https://pythondata.com/wp-content/uploads/2016/11/Screen-Shot-2016-11-24-at-6.52.24-PM-168x300.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/monthly.ao.index.b50.current.ascii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_table(url, sep='\\s+', \n",
    "               parse_dates={'Dates':[0, 1]}, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Standard Pandas Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Dates', 'AO']\n",
    "df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.AO.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.compute().plot(x=\"Dates\", y=\"AO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df.AO > 0]\n",
    "df2.compute().plot(x=\"Dates\", y=\"AO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[df.Dates > '1995-01-01'].AO.sum()\n",
    "print(df3.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize Code with `dask.delayed`\n",
    "\n",
    "- We want to parallelize a simple for-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(0.2)\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    time.sleep(0.2)\n",
    "    return 2 * x\n",
    "\n",
    "def add(x,y):\n",
    "    time.sleep(0.2)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the above functions within two for-loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "\n",
    "n = 20\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = inc(x)\n",
    "    z = double(y)\n",
    "    out.append(z)\n",
    "    \n",
    "total = 0\n",
    "for z in out:\n",
    "    total = add(total, z)\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `dask.delayed` decorator to parallelize the functions `inc`, `double` and `add`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = dask.delayed(inc)\n",
    "double = dask.delayed(double)\n",
    "add = dask.delayed(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `visualize` method (relies on the `graphviz` package) that provide a visual representation of the operations being performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)\n",
    "z.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now revisit the orginal code by using the wrapped/decorated methods defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "data = [i+1 for i in range(n)]\n",
    "\n",
    "out = []\n",
    "for x in data:\n",
    "    y = inc(x)\n",
    "    z = double(y)\n",
    "    out.append(z)\n",
    "    \n",
    "total = 0\n",
    "for z in out:\n",
    "    total = add(total, z)\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we have not physically calculated total yet.\n",
    "- We need to apply the `compute` method to get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "dask.compute(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the visual representation of the sequence of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Pandas DataFrame with 100K rows and two columns with values selected randomly between 1 and 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'X':np.random.randint(1000, size=100000),\n",
    "                   'Y':np.random.randint(1000, size=100000)})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that computes the sum of square for each column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_squares(df):\n",
    "    return df.X**2+df.Y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time it takes to call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df['add_squares'] = df.apply(add_squares,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallelize using Dask `Map_Partition`**\n",
    "\n",
    "- We construct a dask dataframe from pandas dataframe using `from_pandas` function and specify the number of partitions (nparitions) to break this dataframe into.\n",
    "- We will break into 4 partitions (number of available cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.from_pandas(df, npartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply `add_squares` method on each of these partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ddf['z'] = ddf.map_partitions(add_squares, \n",
    "                               meta=(None, 'int64')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myfunc(x, y):\n",
    "    return y * (x**2 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df1 = df.apply(lambda row: myfunc(row.X, row.Y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "ddf = dd.from_pandas(df, npartitions=4*multiprocessing.cpu_count())\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ddfz = ddf.map_partitions(lambda data: \n",
    "                              data.apply(lambda row: myfunc(row.X, row.Y), axis=1)).compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adv_viz.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
