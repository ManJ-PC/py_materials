{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<CENTER>\n",
    "<H1 style=\"color:red\">\n",
    "Accelerating (Numba) and Scaling (Dask) Python Codes on GPUs\n",
    "</H1>\n",
    "</CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>What will be Covered?</font>\n",
    "\n",
    "* What is Numba?\n",
    "* How Does Numba Work?\n",
    "* Numpy and Numba\n",
    "* How to Use Numba?\n",
    "* Parallelization with Numba\n",
    "* Numba and Pandas\n",
    "* Using Numba on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Reference Documents</font>\n",
    "\n",
    "- <a href=\"https://nyu-cds.github.io/python-numba/05-cuda/\">Introduction to Numba: CUDA Programming</a>\n",
    "- <a href=\"https://people.duke.edu/~ccc14/sta-663/CUDAPython.html\">Massively parallel programming with GPUs</a>\n",
    "- <a href=\"https://www.kdnuggets.com/2019/07/accelerate-data-science-on-gpu.html\">Here’s how you can accelerate your Data Science on GPU</a>\n",
    "- \n",
    "- \n",
    "- <a href=\"http://numba.pydata.org/\">Numba: A High Performance Python Compiler</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=UaFSnaYh2b8\">Understanding Numba - the Python and Numpy Compiler</a> (video)\n",
    "- <a href=\"https://examples.dask.org/applications/stencils-with-numba.html\">Stencil Computations with Numba</a>\n",
    "- <a href=\"http://deepdata.com.pl/numba.html\">Python on steroids - speeding up calculations with numba</a>\n",
    "- <a href=\"https://colab.research.google.com/github/evaneschneider/parallel-programming/blob/master/COMPASS_gpu_intro.ipynb\">Introduction to GPU programming with Numba</a>\n",
    "- <a href=\"https://thedatafrog.com/en/articles/make-python-fast-numba/\">Make python fast with numba</a>\n",
    "- <a href=\"https://www.deeplearningwizard.com/deep_learning/production_pytorch/speed_optimization_basics_numba/\">Speed Optimization Basics: Numba</a>\n",
    "- <a href=\"https://murillogroupmsu.com/numba-versus-c/\">High-Performance Python: Why?</a>\n",
    "- <a href=\"https://flothesof.github.io/optimizing-python-code-numpy-cython-pythran-numba.html\">Optimizing your code with NumPy, Cython, pythran and numba </a>\n",
    "- <a href=\"https://www.polymorphe.org/index.php/looping-over-pandas-data-mkd\">Looping over Pandas data</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>What is Numba?</font>\n",
    "\n",
    "> Numba is an open-source JIT compiler that translates a subset of Python and NumPy into fast machine code using `LLVM` (low-level virtual machine), via the llvmlite Python package. It offers a range of options for parallelising Python code for CPUs and GPUs, often with only minor code changes. \n",
    ">\n",
    ">Wikipedia\n",
    "\n",
    "The diagram below, shows all the steps carried out by Numba to execute `do_math`. \n",
    "\n",
    "![fig_numba](https://miro.medium.com/max/1400/1*S0S4QUjR-BsdTICtT9797Q.png)\n",
    "Image Source: Continuum Analytics\n",
    "\n",
    "- **IR**: Intermediate Representations\n",
    "- **Bytecode Analysis**: Intermediate code more abstract than machine code\n",
    "- **LLVM**: Low Level Virtual Machine, infrastructure to develop compilers\n",
    "- **NVVM**: It is an IR compiler based on LLVM, it is designed to represent GPU kernels\n",
    "\n",
    "\n",
    "    \n",
    "**Usage**:\n",
    "\n",
    "- Numba provides several utilities for code generation.\n",
    "- Its central feature is the `numba.jit()` decorator. \n",
    "- Using this decorator, you can mark a function for optimization by Numba’s JIT compiler. - - - Various invocation modes trigger differing compilation options and behaviours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "from numba import njit\n",
    "from numba import prange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking your System**\n",
    "\n",
    "The `numba -s` or `numba --sysinfo` command prints a lot of information about your system and your Numba installation and relevant dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> GPUs</font>\n",
    "\n",
    "![GPUs](http://www.nvidia.com/docs/IO/143716/how-gpu-acceleration-works.png)\n",
    "Image Source: NVIDIA\n",
    "\n",
    "- Graphics Processing Units (GPUs) are custom designed to be very efficient at handling computer graphics and image processing.\n",
    "- Central Processing Units (CPUs) handle computations serially, meaning the logic in handled in one stream: the next task will complete when the subsequent task has finished. CPUs can execute tasks in parallel across cores. For example, most computer CPUs tend to have either two, four or six cores.\n",
    "- In comparison, GPUs have hundreds of 'cores'. This massively parallel architecture is what gives the GPU its high compute performance.\n",
    "\n",
    "**Useful Terminology**\n",
    "\n",
    "| Term | Meaning |\n",
    "| ---  | --- |\n",
    "| `host` | the CPU |\n",
    "| `device` | the GPU |\n",
    "| `host memory` | the system main memory |\n",
    "| `device memory` | onboard memory on a GPU card |\n",
    "| `kernels` | a GPU function launched by the host and executed on the device |\n",
    "| `device function` | a GPU function executed on the device which can only be called from the device  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Numba and GPUs**\n",
    "\n",
    "- Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. \n",
    "- Kernels written in Numba appear to have direct access to NumPy arrays. \n",
    "- NumPy arrays are transferred between the CPU and the GPU automatically.\n",
    "\n",
    "**Dask and GPUs**\n",
    "\n",
    "- Dask can help to scale out large array and dataframe computations by combining the Dask Array and DataFrame collections with a GPU-accelerated array or dataframe library.\n",
    "- The RAPIDS libraries provide a GPU accelerated Pandas-like library, `cuDF`, which interoperates well and is tested against Dask DataFrame.\n",
    "- You can convert a Pandas-backed Dask DataFrame to a cuDF-backed Dask DataFrame.\n",
    "\n",
    "![rapids](https://pbs.twimg.com/media/D2CeyaYVAAAe3kM.jpg)\n",
    "Image Source: NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing the GPU on Google Colab\n",
    "\n",
    "In order to access GPUs for free:\n",
    "\n",
    "1. Go to the `Runtime` menu,\n",
    "2. Click on `Change runtime type`, and \n",
    "3. In the pop-up box, under `Hardware accelerator`, select `GPU` and click on `SAVE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Sanity Check #\n",
    "\n",
    "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
    "\n",
    "Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "device_name = pynvml.nvmlDeviceGetName(handle)\n",
    "\n",
    "if device_name != b'Tesla T4':\n",
    "    raise Exception(\"\"\"\n",
    "                    Unfortunately this instance does not have a T4 GPU.\n",
    "                    Please make sure you've configured Colab to request \n",
    "                    a GPU instance type. \n",
    "                    Sometimes Colab allocates a Tesla K80 instead of a T4. \n",
    "                    Resetting the instance.\n",
    "                    If you get a K80 GPU, try Runtime -> Reset all runtimes...\n",
    "  \"\"\")\n",
    "else:\n",
    "    print('Great! You got the right kind of GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intall RAPIDS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!bash rapidsai-csp-utils/colab/rapids-colab.sh stable\n",
    "\n",
    "import sys, os\n",
    "\n",
    "dist_package_index = sys.path.index('/usr/local/lib/python3.6/dist-packages')\n",
    "sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.6/site-packages'] + sys.path[dist_package_index:]\n",
    "sys.path\n",
    "exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Average Calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def average_sqrt(Mat, avg):\n",
    "    \"\"\"\n",
    "        Average of the square root of all the entries of an array using loops.\n",
    "    \"\"\"\n",
    "    avg = 0.\n",
    "    for x in Mat:\n",
    "        avg += math.sqrt(x)\n",
    "    avg = avg/len(Mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def average_sqrt_numba(Mat, avg):\n",
    "    \"\"\"\n",
    "        Average of the square root of all the entries of an array using loops.\n",
    "    \"\"\"\n",
    "    s = 0.\n",
    "    for x in Mat:\n",
    "        avg += math.sqrt(x)\n",
    "    avg = avg/len(Mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to specify the `cuda` target as argument of the decorator. \n",
    "Numba will automatically:\n",
    "\n",
    "- Compile a Cuda kernel to execute the operations.\n",
    "- Allocate GPU memory for the input.\n",
    "- Execute the CUDA kernel with the correct kernel dimensions given the input sizes.\n",
    "- Copy the result back from the GPU to the CPU.\n",
    "- Return the result on the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(target =\"cuda\")\n",
    "def average_sqrt_cuda(Mat, avg):\n",
    "    \"\"\"\n",
    "        Average of the square root of all the entries of an array using loops.\n",
    "    \"\"\"\n",
    "    avg = 0.\n",
    "    for x in Mat:\n",
    "        avg += math.sqrt(x)\n",
    "    avg = avg/len(Mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1000000\n",
    "Mat = np.random.rand(M)\n",
    "avg = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_reg = %timeit -o average_sqrt(Mat, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_numba = %timeit -o average_sqrt_numba(Mat, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cuda1 = %timeit -o average_sqrt_cuda(Mat, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Regular Speedup: {}\".format(time_reg.best/time_reg.best))\n",
    "print(\"Numba   Speedup: {}\".format(time_reg.best/time_numba.best))\n",
    "print(\"Cuda    Speedup: {}\".format(time_reg.best/time_cuda1.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Cuda Kernels\n",
    "\n",
    "- In CUDA, the code you write will be executed by multiple threads at once (often hundreds or thousands). \n",
    "- The CUDA programming model allows you to abstract the GPU hardware into a software model defined by a thread hierachy that is composed of a **grid** containing **blocks** of **threads**. \n",
    "- These **threads** are the smallest individual unit in the programming model, and they execute together in groups (traditionally called **warps**, consisting of 32 threads each). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Declaration\n",
    "A kernel function is a GPU function that is meant to be called from CPU code. It has two fundamental characteristics:\n",
    "\n",
    "- Kernels cannot explicitly return a value; all result data must be written to an array passed to the function (if computing a scalar, you will probably pass a one-element array).\n",
    "- Kernels explicitly declare their thread hierarchy when called: i.e. the number of thread blocks and the number of threads per block (note that while a kernel is compiled once, it can be called multiple times with different block sizes or grid sizes).\n",
    "\n",
    "```python\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    \"\"\"\n",
    "    Code for kernel.\n",
    "    \"\"\"\n",
    "    # code here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Invocation\n",
    "\n",
    "There are two main steps:\n",
    "\n",
    "- Instantiate the kernel proper, by specifying a number of blocks per grid and a number of threads per block. \n",
    "    - The product of the two will give the total number of threads launched. \n",
    "    - Kernel instantiation is done by taking the compiled kernel function and indexing it with a tuple of integers.\n",
    "- Running the kernel, by passing it the input array (and any separate output arrays if necessary). \n",
    "    - By default, running a kernel is synchronous: the function returns when the kernel has finished executing and the data is synchronized back.\n",
    "\n",
    "A kernel is typically launched in the following way:\n",
    "\n",
    "```python\n",
    "import numpy\n",
    "\n",
    "# Create the data array - usually initialized some other way\n",
    "data = numpy.ones(256)\n",
    "\n",
    "# Set the number of threads in a block\n",
    "threadsperblock = 32 \n",
    "\n",
    "# Calculate the number of thread blocks in the grid\n",
    "blockspergrid = (data.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "# Now start the kernel\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "\n",
    "# Print the result\n",
    "print(data)\n",
    "```\n",
    "\n",
    "\n",
    "**Choosing the Block Size**\n",
    "\n",
    "The two-level thread hierarchy is important for the following reasons:\n",
    "\n",
    "- On the software side, the block size determines how many threads share a given area of shared memory.\n",
    "- On the hardware side, the block size must be large enough for full occupation of execution units; recommendations can be found in the CUDA C Programming Guide.\n",
    "\n",
    "The block size you choose depends on a range of factors, including:\n",
    "\n",
    "- The size of the data array\n",
    "- The size of the shared mempory per block (e.g. 64KB)\n",
    "- The maximum number of threads per block supported by the hardware (e.g. 512 or 1024)\n",
    "- The maximum number of threads per multiprocessor (MP) (e.g. 2048)\n",
    "- The maximum number of blocks per MP (e.g. 32)\n",
    "- The number of threads that can be executed concurrently (a “warp” i.e. 32).\n",
    "\n",
    "\n",
    "Determiming the best size for your grid of thread blocks is a complicated problem that often depends on the specific algorithm and hardware you're using. Here a few good rules of thumb:\n",
    "\n",
    "- The size of a block should be a multiple of 32 threads, with typical block sizes between 128 and 512 threads per block.\n",
    "- The size of the grid should ensure the full GPU is utilized where possible. Launching a grid where the number of blocks is 2x-4x the number of **streaming multiprocessors** on the GPU is a good starting place. (The Tesla K80 GPUs provided by Colaboratory have 15 SMs - more modern GPUs like the P100s on TigerGPU have 60+.)\n",
    "- The CUDA kernel launch overhead does depend on the number of blocks, so it may not be best to launch a grid where the number of threads equals the number of input elements when the input size is very big. We'll show a pattern for dealing with large inputs below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations\n",
    "To achieve the bes performance:\n",
    "\n",
    "- Find ways to parallelize sequential code\n",
    "- Minimize data transfers between the host and the device\n",
    "- Adjust kernel launch configuration to maximize device utilization\n",
    "- Ensure global memory accesses are coalesced\n",
    "- Minimize redundant accesses to global memory whenever possible\n",
    "- Avoid different execution paths within the same warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def average_sqrt_cuda_kernel(x, avg):\n",
    "    tidx = cuda.threadIdx.x # this is the unique thread ID within a 1D block\n",
    "    bidx = cuda.blockIdx.x  # Similarly, this is the unique block ID within the 1D grid\n",
    "\n",
    "    block_dimx = cuda.blockDim.x  # number of threads per block\n",
    "    grid_dimx = cuda.gridDim.x    # number of blocks in the grid\n",
    "    \n",
    "    start = tidx + bidx * block_dimx\n",
    "    stride = block_dimx * grid_dimx\n",
    "    \n",
    "    avg = 0.\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        avg += np.sqrt(x[i])\n",
    "    avg = avg/x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_per_block = 128\n",
    "blocks_per_grid = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cuda2 = %timeit -o average_sqrt_cuda_kernel([blocks_per_grid, threads_per_block](Mat, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cuda Kernel Speedup: {}\".format(time_reg.best/time_cuda2.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "A = np.random.rand(N, N)\n",
    "B = np.random.rand(N, N)\n",
    "C = np.zeros_like(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiplication(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            tmp = 0.\n",
    "            for k in range(n):\n",
    "                tmp  += A[i, k]*B[k, j]\n",
    "            C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRegMat = %timeit -o matrix_multiplication(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def matrix_multiplication_numba(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in prange(n):\n",
    "        for j in prange(n):\n",
    "            tmp = 0.\n",
    "            for k in prange(n):\n",
    "                tmp += A[i, k]*B[k, j]\n",
    "            C[i,j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNumMat = %timeit -o matrix_multiplication_numba(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matrix_multiplication_cuda(A, B, C):\n",
    "    \"\"\"\n",
    "      Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tCudaMat1 = %timeit -o matrix_multiplication_cuda(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Numba : {}\".format(tRegMat.best/tNumMat.best))\n",
    "print(\"Speedup CUDA 1:  {}\".format(tRegMat.best/tCudaMat1.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be a faster way for the matrix multiplication with Cuda:\n",
    "\n",
    "- Each thread block is responsible for computing a square sub-matrix of C and each thread for computing an element of the sub-matrix. \n",
    "- The sub-matrix is equal to the product of a square sub-matrix of A (sA) and a square sub-matrix of B (sB). \n",
    "- In order to fit into the device resources, the two input matrices are divided into as many square sub-matrices of dimension TPB as necessary, and the result computed as the sum of the products of these square sub-matrices.\n",
    "- Each product is performed by first loading sA and sB from global memory to shared memory, with one thread loading each element of each sub-matrix. \n",
    "- Once sA and sB have been loaded, each thread accumulates the result into a register (tmp). Once all the products have been calculated, the results are written to the matrix C in global memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float32\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matrix_multiplication_cuda_fast(A, B, C):\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tCudaMat2 = %timeit -o matrix_multiplication_cuda_fast(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Numba 2: {}\".format(tRegMat.best/tNumMat2.best))\n",
    "print(\"Speedup CUDA 1:  {}\".format(tRegMat.best/tCudaMat1.best))\n",
    "print(\"Speedup CUDA 2:  {}\".format(tRegMat.best/tCudaMat2.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandelbrot Fractal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pure Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color function for point at (x, y)\n",
    "def mandel(x, y, max_iters):\n",
    "    c = complex(x, y)\n",
    "    z = 0.0j\n",
    "    for i in range(max_iters):\n",
    "        z = z*z + c\n",
    "        if z.real*z.real + z.imag*z.imag >= 4:\n",
    "            return i\n",
    "    return max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fractal(xmin, xmax, ymin, ymax, image, iters):\n",
    "    height, width = image.shape\n",
    "\n",
    "    pixel_size_x = (xmax - xmin)/width\n",
    "    pixel_size_y = (ymax - ymin)/height\n",
    "\n",
    "    for x in range(width):\n",
    "        real = xmin + x*pixel_size_x\n",
    "        for y in range(height):\n",
    "            imag = ymin + y*pixel_size_y\n",
    "            color = mandel(real, imag, iters)\n",
    "            image[y, x]  = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gimage = np.zeros((1024, 1536), dtype=np.uint8)\n",
    "xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype('float32')\n",
    "iters = 50\n",
    "\n",
    "start = timer()\n",
    "create_fractal(xmin, xmax, ymin, ymax, gimage, iters)\n",
    "dt_py = timer() - start\n",
    "\n",
    "print(\"Mandelbrot created on CPU in {} s\".format(dt_py))\n",
    "plt.imshow(gimage);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numba Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandel_numba = jit(nb.uint32(nb.float32, nb.float32, nb.uint32))(mandel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def create_fractal_numba(xmin, xmax, ymin, ymax, image, iters):\n",
    "    height, width = image.shape\n",
    "\n",
    "    pixel_size_x = (xmax - xmin)/width\n",
    "    pixel_size_y = (ymax - ymin)/height\n",
    "\n",
    "    for x in range(width):\n",
    "        real = xmin + x*pixel_size_x\n",
    "        for y in range(height):\n",
    "            imag = ymin + y*pixel_size_y\n",
    "            color = mandel_numba(real, imag, iters)\n",
    "            image[y, x]  = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gimage = np.zeros((1024, 1536), dtype=np.uint8)\n",
    "xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype('float32')\n",
    "iters = 50\n",
    "\n",
    "start = timer()\n",
    "create_fractal_numba(xmin, xmax, ymin, ymax, gimage, iters)\n",
    "dt_numba = timer() - start\n",
    "\n",
    "print(\"Mandelbrot created on CPU in {} s\".format(dt_numba))\n",
    "plt.imshow(gimage);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandel_gpu = cuda.jit(restype=nb.uint32, \n",
    "                      argtypes=[nb.float32, nb.float32, nb.uint32], \n",
    "                      device=True)(mandel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(argtypes=[nb.float32, nb.float32, nb.float32, nb.float32, nb.uint8[:,:], nb.uint32])\n",
    "def create_fractal_kernel(xmin, xmax, ymin, ymax, image, iters):\n",
    "    height, width = image.shape\n",
    "\n",
    "    pixel_size_x = (xmax - xmin)/width\n",
    "    pixel_size_y = (ymax - ymin)/height\n",
    "\n",
    "    startX, startY = cuda.grid(2)\n",
    "    gridX = cuda.gridDim.x * cuda.blockDim.x # stride in x\n",
    "    gridY = cuda.gridDim.y * cuda.blockDim.y # stride in y\n",
    "\n",
    "    for x in range(startX, width, gridX):\n",
    "        real = xmin + x*pixel_size_x\n",
    "        for y in range(startY, height, gridY):\n",
    "            imag = ymin + y*pixel_size_y\n",
    "            color = mandel_gpu(real, imag, iters)\n",
    "            image[y, x]  = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gimage = np.zeros((1024, 1536), dtype=np.uint8)\n",
    "blockdim = (32, 8)\n",
    "griddim = (32, 16)\n",
    "xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype('float32')\n",
    "iters = 50\n",
    "\n",
    "start = timer()\n",
    "d_image = cuda.to_device(gimage)\n",
    "create_fractal_kernel[griddim, blockdim](xmin, xmax, ymin, ymax, d_image, iters)\n",
    "d_image.to_host()\n",
    "dt_cuda = timer() - start\n",
    "\n",
    "print(\"Mandelbrot created on GPU in {} s\".format(dt_cuda))\n",
    "plt.imshow(gimage);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup CPU:    {}\".format(dt_py/dt_py))\n",
    "print(\"Speedup Numba:  {}\".format(dt_py/dt_numba))\n",
    "print(\"Speedup CUDA:   {}\".format(dt_py/dt_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [cuDF and Dask-cuDF](https://docs.rapids.ai/api/cudf/stable/10min.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask_cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "\n",
    "print('Dask cuDF Version:', dask_cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `cudf` to create a dataframe and perform operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 100000\n",
    "df = cudf.DataFrame({'X':np.random.randint(1000, size=num_rows),\n",
    "                     'Y':np.random.randint(1000, size=num_rows)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_squares(df):\n",
    "    return df.X**2 + df.Y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['add_squares'] = add_squares(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same as above using `dask_cudf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask_cudf.from_cudf(df, npartitions=4)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf['z'] = add_squares(ddf).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time Series Data**\n",
    "\n",
    "`DataFrames` supports `datetime` typed columns, which allow users to interact with and filter data based on specific timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "date_df = cudf.DataFrame()\n",
    "date_df['date'] = pd.date_range('01/05/1980', periods=15000, freq='D')\n",
    "date_df['value'] = cp.random.sample(len(date_df))\n",
    "date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_date1 = dt.datetime.strptime('2001-09-11', '%Y-%m-%d')\n",
    "search_date2 = dt.datetime.strptime('2019-11-23', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "date_df.query('date >= @search_date1' and 'date <= @search_date2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ddf = dask_cudf.from_cudf(date_df, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "date_ddf.query('date >= @search_date1' and 'date <= @search_date2', \n",
    "               local_dict={'search_date1':search_date1, \n",
    "                           'search_date2':search_date2}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
