{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<CENTER>\n",
    "<H1 style=\"color:red\">\n",
    "Introduction to Numba\n",
    "</H1>\n",
    "</CENTER>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I’m becoming more and more convinced that Numba is the future of fast scientific computing in Python. \n",
    ">\n",
    "> – Jake Vanderplas, 2013-06-15\n",
    ">\n",
    "> http://jakevdp.github.io/blog/2013/06/15/numba-vs-cython-take-2/\n",
    "\n",
    "\n",
    "![fig_numba](https://thedatafrog.com/static/blog/images/2019/07/python_fast.0d88afcb4f8a.png)\n",
    "Image Source: Lison Bernet 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>What will be Covered?</font>\n",
    "\n",
    "* What is Numba?\n",
    "* How Does Numba Work?\n",
    "* Numpy and Numba\n",
    "* How to Use Numba?\n",
    "* Parallelization with Numba\n",
    "* Numba and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Reference Documents</font>\n",
    "- <a href=\"http://numba.pydata.org/\">Numba: A High Performance Python Compiler</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=UaFSnaYh2b8\">Understanding Numba - the Python and Numpy Compiler</a> (video)\n",
    "- <a href=\"https://examples.dask.org/applications/stencils-with-numba.html\">Stencil Computations with Numba</a>\n",
    "- <a href=\"http://deepdata.com.pl/numba.html\">Python on steroids - speeding up calculations with numba</a>\n",
    "- <a href=\"https://colab.research.google.com/github/evaneschneider/parallel-programming/blob/master/COMPASS_gpu_intro.ipynb\">Introduction to GPU programming with Numba</a>\n",
    "- <a href=\"https://thedatafrog.com/en/articles/make-python-fast-numba/\">Make python fast with numba</a>\n",
    "- <a href=\"https://www.deeplearningwizard.com/deep_learning/production_pytorch/speed_optimization_basics_numba/\">Speed Optimization Basics: Numba</a>\n",
    "- <a href=\"https://murillogroupmsu.com/numba-versus-c/\">High-Performance Python: Why?</a>\n",
    "- <a href=\"https://flothesof.github.io/optimizing-python-code-numpy-cython-pythran-numba.html\">Optimizing your code with NumPy, Cython, pythran and numba </a>\n",
    "- <a href=\"https://www.polymorphe.org/index.php/looping-over-pandas-data-mkd\">Looping over Pandas data</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>What is Numba?</font>\n",
    "\n",
    "> Numba is an open-source JIT compiler that translates a subset of Python and NumPy into fast machine code using `LLVM`, via the llvmlite Python package. It offers a range of options for parallelising Python code for CPUs and GPUs, often with only minor code changes. \n",
    ">\n",
    ">Wikipedia\n",
    "\n",
    "- Numba is a Python open source package that was originally developed by Continuum Analytics.\n",
    "- The core application area are math-heavy and array-oriented functions, which are in native Python pretty slow.\n",
    "- From a function, Numba can generate native code for that function as well as the wrapper code needed to call it directly from Python. This compilation is done on-the-fly and in-memory.\n",
    "- It accelerates Python code (numerical functions) for both CPU and GPU:\n",
    "   - **Function Compiler**: Numba compiles Python functions, not whole applications or parts of it. It is a Python module meant to improve the performance of functions with the goal of achieving a speed comparable to `C`.\n",
    "   - **Just-in-time**: (Dynamic translation) Numba translates the bytecode (intermediate code more abstract than the machine code) to machine code immediately before its execution to improve the execution speed.\n",
    "   - **Numerically-focused**: Numba is focused on numerical data, such as int, float, complex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>How Does Numba Work?</font>\n",
    "\n",
    "- Assume that you have a function `do_math` that is decorated with the Numba `@jit` decorator. \n",
    "- Compilation will be deferred until the first function execution. \n",
    "- Numba will infer the argument types at call time, and generate optimized code based on this information. \n",
    "- Numba will also be able to compile separate specializations depending on the input types. \n",
    "- The diagram below, shows all the steps carried out by Numba to execute `do_math`. \n",
    "\n",
    "![fig_numba](https://miro.medium.com/max/1400/1*S0S4QUjR-BsdTICtT9797Q.png)\n",
    "Image Source: Continuum Analytics\n",
    "\n",
    "- **IR**: Intermediate Representations\n",
    "- **Bytecode Analysis**: Intermediate code more abstract than machine code\n",
    "- **LLVM**: Low Level Virtual Machine, infrastructure to develop compilers\n",
    "- **NVVM**: It is an IR compiler based on LLVM, it is designed to represent GPU kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Numpy and Numba</font>\n",
    "- One objective of Numba is having a seamless integration with NumPy. \n",
    "- Numba excels at generating code that executes on top of NumPy arrays.\n",
    "- NumPy support in Numba comes in many forms:\n",
    "    1. Numba understands calls to NumPy ufuncs (universal functions: there are over 60 of them) and is able to generate equivalent native code for many of them.\n",
    "    2. NumPy arrays are directly supported in Numba.\n",
    "    3. Numba is able to generate ufuncs and gufuncs (generalized universal functions). This means that it is possible to implement ufuncs and gufuncs within Python, getting speeds comparable to that of ufuncs/gufuncs implemented in C extension modules using the NumPy C API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Usage</font>\n",
    "- Numba provides several utilities for code generation.\n",
    "- Its central feature is the `numba.jit()` decorator. \n",
    "- Using this decorator, you can mark a function for optimization by Numba’s JIT compiler. - - - Various invocation modes trigger differing compilation options and behaviours.\n",
    "\n",
    "\n",
    "Consider using Numba if:\n",
    "\n",
    "- Is numerically orientated.\n",
    "- Uses Numpy\n",
    "- Relies on loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "from numba import njit\n",
    "from numba import prange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking your System**\n",
    "\n",
    "The `numba -s` or `numba --sysinfo` command prints a lot of information about your system and your Numba installation and relevant dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Consider the function that multiplies two `nxn` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiplication(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            tmp = 0.\n",
    "            for k in range(n):\n",
    "                tmp  += A[i, k]*B[k, j]\n",
    "            C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "A = np.random.rand(N, N)\n",
    "B = np.random.rand(N, N)\n",
    "C = np.zeros((N, N))\n",
    "D = np.random.rand(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRegMat = %timeit -o matrix_multiplication(A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to use `Numba`:\n",
    "\n",
    "- Method 1: As a function calling the function we want to speed\n",
    "- Method 2: As a decorator of the function we want to speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba_matrix_multiplication = jit(matrix_multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNumMat0 = %timeit -o numba_matrix_multiplication(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Numba 0: {}\".format(tRegMat.best/tNumMat0.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def matrix_multiplication_numba(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            tmp = 0.\n",
    "            for k in range(n):\n",
    "                tmp  += A[i, k]*B[k, j]\n",
    "            C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit matrix_multiplication_numba(A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Measuring the Performance of Numba**\n",
    "\n",
    "- Once the compilation has taken place, Numba runs the machine code version of your function. \n",
    "- If it is called again with same argument types, it can reuse the cached version instead of having to compile again.\n",
    "- A common mistake when measuring performance is not accounting for the above behaviour and to time code once with a simple timer that includes the time taken to compile your function in the execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT REPORT THIS... COMPILATION TIME IS INCLUDED IN THE EXECUTION TIME!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_1 = time.time()\n",
    "matrix_multiplication_numba(A, B, C)\n",
    "end_1 = time.time()\n",
    "print(\"Elapsed (with compilation) = %s\" % (end_1 - start_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW THE FUNCTION IS COMPILED, RE-TIME IT EXECUTING FROM CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_2 = time.time()\n",
    "matrix_multiplication_numba(A, B, C)\n",
    "end_2 = time.time()\n",
    "print(\"Elapsed (after compilation) = %s\" % (end_2 - start_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Signature\n",
    "\n",
    "- You can specify the signature of the Numba function by describing the types of the arguments and the return type of the function. \n",
    "- This can produce slightly faster code as the compiler does not need to infer the types. \n",
    "- The drawback is that the function can no longer accept other types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_numbers(x, y):\n",
    "    return (x + y)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba_average_numbers = jit(nb.float64(nb.int32, nb.int32))(average_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nb.float64(nb.int32, nb.int32))\n",
    "def average_numbers_numba(x, y):\n",
    "    return (x + y)/2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nb.float64(nb.int32, nb.int32)` is the function’s signature specifying a function that takes two 32-bit integer arguments and returns a double precision float.\n",
    "- You can also use the abbreviated notation: `nb.f8(nb.i4, nb.i4)`\n",
    "- If you only pass `(nb.i4, nb.i4)` instead of `nb.f8(nb.i4, nb.i4)`, Numba will try to infer the type of the return value.\n",
    "- Array signatures are specified by subscripting a base type according to the number of dimensions. \n",
    "     - A 1-dimension single-precision array would be written `nb.f4[:]`.\n",
    "     - A 3-dimension array of the same underlying type would be `nb.f4[:,:,:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numba_matrix_multiplication = jit((nb.f8[:,:], nb.f8[:,:], nb.f8[:,:]))(matrix_multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNumMat1 = %timeit -o numba_matrix_multiplication(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Numba 1: {}\".format(tRegMat.best/tNumMat1.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Example: Finding the Closet Two Points**\n",
    "\n",
    "- Find the two closest points in an array of points in 2D. \n",
    "- Returns the two points, and the distance between them.\n",
    "- If we have N points, we would have to test NxN pairs of points. \n",
    "- This algorithm has a complexity of order NxN, denoted O(NxN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def python_closest(points):\n",
    "    min_distance2 = 999999.\n",
    "    mdp1, mdp2 = None, None\n",
    "    for i in range(len(points)):\n",
    "        for j in range(i+1, len(points)):\n",
    "            distance2 = (points[i][0]-points[j][0])**2 + \\\n",
    "                        (points[i][1]-points[j][1])**2\n",
    "            if distance2 < min_distance2:\n",
    "               min_distance2 = distance2\n",
    "               mdp1, mdp2 = points[i], points[j]\n",
    "    return mdp1, mdp2, math.sqrt(min_distance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.random.uniform((-1,-1), (1,1), (8100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit python_closest(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use Numba to speedup the calculations. We can explicitly pass the types of the arguments to have a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "@jit('Tuple((float64[:], float64[:], float64))(float64[:,:])', nopython=True)\n",
    "def numba_closest(points):\n",
    "    min_distance2 = 999999.\n",
    "    mdp1, mdp2 = None, None\n",
    "    for i in prange(len(points)):\n",
    "        for j in prange(i+1, len(points)):\n",
    "            distance2 = (points[i][0]-points[j][0])**2 + \\\n",
    "                        (points[i][1]-points[j][1])**2\n",
    "            if distance2 < min_distance2:\n",
    "               min_distance2 = distance2\n",
    "               mdp1, mdp2 = points[i], points[j]\n",
    "    return mdp1, mdp2, math.sqrt(min_distance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit numba_closest(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilation Options\n",
    "A number of keyword-only arguments can be passed to the `@jit` decorator:\n",
    "1. `nopython`: Numba has two compilation modes:\n",
    "     - **nopython mode** (`nopython=True`): Compile the decorated function so that it will run entirely without the involvement of the Python interpreter. This mode produces the highest performance code, but requires that the native types of all values in the function can be inferred. Note that <font color=\"red\">**`@njit`**</font> is an alias for <font color=\"red\">**`@jit(nopython=True)`**</font>.\n",
    "     - **object mode**: In this mode Numba will identify loops that it can compile and compile those into functions that run in machine code, and it will run the rest of the code in the interpreter. For best performance avoid using this mode!\n",
    "     - By default Numba will automatically use **object mode** if **nopython mode** cannot be used for some reason. \n",
    "     - When you are in **nopython mode**, types that cannot be inferred by the compiler will generate an error.\n",
    "2. `nogil`: \n",
    "     - Whenever Numba optimizes Python code to native code that only works on native types and variables (rather than Python objects), it is not necessary anymore to hold Python’s global interpreter lock (GIL). \n",
    "     - Numba will release the GIL when entering such a compiled function if you passed `nogil=True`.\n",
    "     - When using `nogil=True`, you need to be wary of the usual pitfalls of multi-threaded programming (consistency, synchronization, race conditions, etc.).\n",
    "3. `cache`:\n",
    "     - To avoid compilation times each time you invoke a Python program, you can instruct Numba to write the result of function compilation into a file-based cache. \n",
    "     - This is done by passing `cache=True`.\n",
    "4. `parallel`: \n",
    "     - Enables automatic parallelization (and related optimizations) for operations in the function known to have parallel semantics.\n",
    "     - This feature is enabled by passing `parallel=True` and must be used in conjunction with `nopython=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use Numba (and Dask) to speed up the code below (calculations of `pi`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "def approximate_pi(num_samples):\n",
    "    num_points_circ = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Select an arbitrary point in [-1,1]x[-1,1]\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "\n",
    "        # Check if the point is inside the circle\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            num_points_circ += 1\n",
    "\n",
    "    return 4 * num_points_circ / num_samples\n",
    "\n",
    "def mean(*args):\n",
    "    return sum(args) / len(args)\n",
    "\n",
    "num_samples = 10**6\n",
    "num_experiments = 10\n",
    "\n",
    "pi_approx = mean(*[approximate_pi(num_samples) for i in range(num_experiments)])\n",
    "\n",
    "print(\"Approximation of Pi: {}\".format(pi_approx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastmath\n",
    "- In certain classes of applications strict IEEE 754 compliance is less important. \n",
    "- It is possible to relax some numerical rigour with view of gaining additional performance. \n",
    "- The way to achieve this behaviour in Numba is through the use of the `fastmath` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=False)\n",
    "def do_sum(A):\n",
    "    acc = 0.\n",
    "    # without fastmath, this loop must accumulate in strict order\n",
    "    for x in A:\n",
    "        acc += np.sqrt(x)\n",
    "    return acc\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def do_sum_fast(A):\n",
    "    acc = 0.\n",
    "    # with fastmath, the reduction can be vectorized as floating point\n",
    "    # reassociation is permitted.\n",
    "    for x in A:\n",
    "        acc += np.sqrt(x)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_do_sum = %timeit -o acc1 = do_sum(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_do_sum_fast = %timeit  -o acc2 = do_sum_fast(D)\n",
    "print(time_do_sum.best / time_do_sum_fast.best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Parallelization </font>\n",
    "\n",
    "- The setting `parallel=True` in `jit()` enables a Numba transformation pass that attempts to automatically parallelize and perform other optimizations on (part of) a function.\n",
    "- A user program may contain operations (for instance adding a scalar value to an array) that are known to have parallel semantics.\n",
    "- Each operation could be parallelized individually but that might light to poor performance due to poor cache behavior.\n",
    "- Numba uses instead auto-parallelization where it identifies all operations with parallel sementics and fuses adjacent ones together, to form one or more kernels that are automatically run in parallel.\n",
    "- The process is fully automated without modifications to the user program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Parallel Loops\n",
    "\n",
    "- Numba parallel execution also has support for explicit parallel loop declaration similar to that in OpenMP. \n",
    "- To indicate that a loop should be executed in parallel the `numba.prange` function should be used.\n",
    "- This function behaves like Python `range` and if `parallel=True` is not set it acts simply as an alias of `range`. \n",
    "- Loops induced with `prange` can be used for embarrassingly parallel computation and also reductions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def matrix_multiplication_numba2(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in prange(n):\n",
    "        for j in prange(n):\n",
    "            tmp = 0.\n",
    "            for k in prange(n):\n",
    "                tmp += A[i, k]*B[k, j]\n",
    "            C[i,j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNumMat2 = %timeit -o matrix_multiplication_numba2(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Numba 2: {}\".format(tRegMat.best/tNumMat2.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_functions(n):\n",
    "    \"\"\"\n",
    "        Evaluate the trigononmetric functions for n values evenly\n",
    "        spaced over the interval [-1500.00, 1500.00]\n",
    "    \"\"\"\n",
    "    vector1 = np.linspace(-1500.00, 1500.0, n)\n",
    "    iterations = 10000\n",
    "    for i in range(iterations):\n",
    "        vector2 = np.sin(vector1)\n",
    "        vector1 = np.arcsin(vector2)\n",
    "        vector2 = np.cos(vector1)\n",
    "        vector1 = np.arccos(vector2)\n",
    "        vector2 = np.tan(vector1)\n",
    "        vector1 = np.arctan(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def evaluate_functions_numba(n):\n",
    "    \"\"\"\n",
    "        Evaluate the trigononmetric functions for n values evenly\n",
    "        spaced over the interval [-1500.00, 1500.00]\n",
    "    \"\"\"\n",
    "    vector1 = np.linspace(-1500.00, 1500.0, n)\n",
    "    iterations = 10000\n",
    "    for i in prange(iterations):\n",
    "        vector2 = np.sin(vector1)\n",
    "        vector1 = np.arcsin(vector2)\n",
    "        vector2 = np.cos(vector1)\n",
    "        vector1 = np.arccos(vector2)\n",
    "        vector2 = np.tan(vector1)\n",
    "        vector1 = np.arctan(vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRegFun = %timeit -o evaluate_functions(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNumFun = %timeit -o evaluate_functions_numba(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup: {}\".format(tRegFun.best/tNumFun.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics\n",
    "- We can produce diagnostic information about the transforms undertaken in automatically parallelizing the decorated code. \n",
    "- This information can be accessed in two ways:\n",
    "     1. Setting the environment variable: `NUMBA_PARALLEL_DIAGNOSTICS`\n",
    "     2. Calling the function `parallel_diagnostics()`\n",
    "- The level of verbosity in the diagnostic information is controlled by an integer argument of value between 1 and 4 inclusive, 1 being the least verbose and 4 the most.\n",
    "\n",
    "For additional information, consult the webpage: <a href=\"http://numba.pydata.org/numba-doc/latest/user/parallel.html\"> http://numba.pydata.org/numba-doc/latest/user/parallel.html</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_functions_numba.parallel_diagnostics(level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Calling Other Functions</font>\n",
    "\n",
    "- Numba functions can call other Numba functions. \n",
    "- Both functions must have the `@jit` decorator, otherwise the code will be much slower.\n",
    "\n",
    "```python\n",
    "@jit\n",
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    return math.sqrt(square(x) + square(y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Numba and Dask</font>\n",
    "\n",
    "- Numba allows for run-time compilations of functions to optimize single-machine code.\n",
    "    - If you intend to call a function multiple times, you can decrease your compute time significantly by compliling the function on the first call. \n",
    "    - Numba is useful for speeding up individual tasks.\n",
    "- Dask is a parallel computing library for out-of-memory and distributed computations. \n",
    "    - At the heart of dask are a series of task schedulers — algorithms for determining when and how to run various user-defined computational “tasks”; consequently, dask can automatically identify which tasks can be run in parallel, or not run at all. \n",
    "    - Employing dask’s schedulers allows us to scale out to a network of many interrelated tasks and efficiently compute only those outputs we need, even on a single machine.\n",
    "    \n",
    "**Example**\n",
    "\n",
    "Use Numba and Dask for the approximation of Pi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "import dask\n",
    "\n",
    "@dask.delayed\n",
    "@nb.jit(nopython=True, nogil=True)\n",
    "def approximate_pi(num_samples):\n",
    "    num_points_circ = 0\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Select an arbitrary point in [-1,1]x[-1,1]\n",
    "        x = random.uniform(-1, 1)\n",
    "        y = random.uniform(-1, 1)\n",
    "\n",
    "        # Check if the point is inside the circle\n",
    "        if x**2 + y**2 < 1.0:\n",
    "            num_points_circ += 1\n",
    "\n",
    "    return 4 * num_points_circ / num_samples\n",
    "\n",
    "@dask.delayed\n",
    "def mean(*args):\n",
    "    return sum(args) / len(args)\n",
    "\n",
    "number_samples = 10**6\n",
    "number_experiments = 10\n",
    "\n",
    "pi_approx = mean(*[approximate_pi(number_samples) for i in range(number_experiments)])\n",
    "\n",
    "print(\"Approximation of Pi: {}\".format(pi_approx.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Numba and Pandas</font>\n",
    "\n",
    "- Pandas is built on top of Numpy.\n",
    "- Pandas offers flexibility in manipulating data but not necessary speed.\n",
    "- This flexibility allows the creation of built-in function.\n",
    "- Crude looping (over DataFrame rows for instance) in Pandas does not take advantage of any built-in optimizations, making it extremely inefficient.\n",
    "- Using vectorized Pandas built-in functions (acting on Pandas Series) is almost always preferable to accomplishing similar ends with custom-written looping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "- We have a Pandas DataFrame and we want to add a new column by multiplying an exiting column by a constant.\n",
    "- We use three methods methods for the multiplication operations: `apply` method, Pandas and vectorization with Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def multiply(x):\n",
    "    return x * 5\n",
    "    \n",
    "@nb.vectorize\n",
    "def multiply_numba(x):\n",
    "    return x * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table of 100,000 rows and 4 columns filled with random numbers from 0 to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(0,100,size=(100000, 4)),columns=['a', 'b', 'c', 'd'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_apply = %timeit -o df['new_col1'] = df['a'].apply(multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pandas = %timeit -o df['new_col2'] = df['a'] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_numba1 = %timeit -o df['new_col3'] = multiply_numba(df['a'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multiply Apply:  {}\".format(time_apply.best/time_apply.best))\n",
    "print(\"Multiply Pandas: {}\".format(time_apply.best/time_pandas.best))\n",
    "print(\"Multiply Numba:  {}\".format(time_apply.best/time_numba1.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "- Square the values of each row and take their mean to create a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_mean(row):\n",
    "    row = np.power(row, 2)\n",
    "    return np.mean(row)\n",
    "\n",
    "@njit\n",
    "def square_mean_numba(arr):\n",
    "    res = np.empty(arr.shape[0])\n",
    "    arr = np.power(arr, 2)\n",
    "    for i in prange(arr.shape[0]):\n",
    "        res[i] = np.mean(arr[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows_list = [10, 100, 1000, 10000, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.04 ms ± 28.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "25.2 ms ± 677 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "257 ms ± 14.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "2.01 s ± 213 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "20.3 s ± 2.88 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "pandas_times = list()\n",
    "for nrows in nrows_list:\n",
    "    df = pd.DataFrame(np.random.randint(0,100,size=(nrows, 2)),columns=['a', 'b'])\n",
    "    tp = %timeit -o df['new_col'] = df.apply(square_mean, axis=1)\n",
    "    pandas_times.append(tp.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 µs ± 2.96 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "115 µs ± 7.46 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "154 µs ± 1.33 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "657 µs ± 64.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "5.87 ms ± 308 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "numba_times = list()\n",
    "for nrows in nrows_list:\n",
    "    df = pd.DataFrame(np.random.randint(0,100,size=(nrows, 2)),columns=['a', 'b'])\n",
    "    tn = %timeit -o df['new_col'] = square_mean_numba(df.to_numpy())\n",
    "    numba_times.append(tn.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002996175549997133, 0.024544068600516767, 0.24745299049973254, 1.789480421000917, 18.07156833499903]\n",
      "[9.9499519100209e-05, 0.00010769562369969208, 0.0001528813461001846, 0.0005910507360022166, 0.005486255939977127]\n"
     ]
    }
   ],
   "source": [
    "print(pandas_times)\n",
    "print(numba_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Time (s)')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(len(nrows_list))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "width = 0.25\n",
    "axes.bar(x, pandas_times, width, label='Pandas apply')\n",
    "axes.bar(x + width, numba_times, width, label='Numba function')\n",
    "axes.set_xticks(x)\n",
    "axes.set_xticklabels(nrows_list)\n",
    "axes.legend(prop={'size': 10})\n",
    "axes.set_yscale('log')\n",
    "axes.set_xlabel('Number of rows')\n",
    "axes.set_ylabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Could we claim that Numpy/Numba is faster than Pandas?**\n",
    "\n",
    "- Not necessarily!\n",
    "- Over time, Pandas relies more on  Cython operations.\n",
    "- In Pandas 1.0 (and newer versions) Pandas’ `apply()` method (applies a function along a specific axis of a DataFrame) can make use of Numba (if installed) instead of cython and be faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Using GPUs</font>\n",
    "\n",
    "- Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. \n",
    "- Kernels written in Numba appear to have direct access to NumPy arrays. \n",
    "- NumPy arrays are transferred between the CPU and the GPU automatically.\n",
    "\n",
    "![GPUs](http://www.nvidia.com/docs/IO/143716/how-gpu-acceleration-works.png)\n",
    "Image Source: NVIDIA\n",
    "\n",
    "**Useful Terminology**\n",
    "\n",
    "| Term | Meaning |\n",
    "| ---  | --- |\n",
    "| `host` | the CPU |\n",
    "| `device` | the GPU |\n",
    "| `host memory` | the system main memory |\n",
    "| `device memory` | onboard memory on a GPU card |\n",
    "| `kernels` | a GPU function launched by the host and executed on the device |\n",
    "| `device function` | a GPU function executed on the device which can only be called from the device  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing the GPU on Google Colab\n",
    "\n",
    "In order to access GPUs for free:\n",
    "\n",
    "1. Go to the `Runtime` menu,\n",
    "2. Click on `Change runtime type`, and \n",
    "3. In the pop-up box, under `Hardware accelerator`, select `GPU` and click on `SAVE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Average Calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def average_sqrt(Mat, avg):\n",
    "    \"\"\"\n",
    "        Average of the square root of all the entries of an array using loops.\n",
    "    \"\"\"\n",
    "    avg = 0.\n",
    "    for x in Mat:\n",
    "        avg += math.sqrt(x)\n",
    "    avg = avg/len(Mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def average_sqrt_numba(Mat, avg):\n",
    "    \"\"\"\n",
    "        Average of the square root of all the entries of an array using loops.\n",
    "    \"\"\"\n",
    "    s = 0.\n",
    "    for x in Mat:\n",
    "        avg += math.sqrt(x)\n",
    "    avg = avg/len(Mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need to specify the `cuda` target as argument of the decorator. \n",
    "Numba will automatically:\n",
    "\n",
    "- Compile a Cuda kernel to execute the operations.\n",
    "- Allocate GPU memory for the input.\n",
    "- Execute the CUDA kernel with the correct kernel dimensions given the input sizes.\n",
    "- Copy the result back from the GPU to the CPU.\n",
    "- Return the result on the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(target =\"cuda\")\n",
    "def average_sqrt_cuda(Mat, avg):\n",
    "    \"\"\"\n",
    "        Average of the square root of all the entries of an array using loops.\n",
    "    \"\"\"\n",
    "    avg = 0.\n",
    "    for x in Mat:\n",
    "        avg += math.sqrt(x)\n",
    "    avg = avg/len(Mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1000000\n",
    "Mat = np.random.rand(M)\n",
    "avg = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_reg = %timeit -o average_sqrt(Mat, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_numba = %timeit -o average_sqrt_numba(Mat, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cuda1 = %timeit -o average_sqrt_cuda(Mat, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Regular Speedup: {}\".format(time_reg.best/time_reg.best))\n",
    "print(\"Numba   Speedup: {}\".format(time_reg.best/time_numba.best))\n",
    "print(\"Cuda    Speedup: {}\".format(time_reg.best/time_cuda1.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Cuda Kernels\n",
    "\n",
    "- The CUDA programming model allows you to abstract the GPU hardware into a software model composed of a **grid** containing **blocks** of **threads**. \n",
    "- These **threads** are the smallest individual unit in the programming model, and they execute together in groups (traditionally called **warps**, consisting of 32 threads each). \n",
    "- Determiming the best size for your grid of thread blocks is a complicated problem that often depends on the specific algorithm and hardware you're using, but here a few good rules of thumb:\n",
    "     - The size of a block should be a multiple of 32 threads, with typical block sizes between 128 and 512 threads per block.\n",
    "     - The size of the grid should ensure the full GPU is utilized where possible. Launching a grid where the number of blocks is 2x-4x the number of **streaming multiprocessors** on the GPU is a good starting place. (The Tesla K80 GPUs provided by Colaboratory have 15 SMs - more modern GPUs like the P100s on TigerGPU have 60+.)\n",
    "     - The CUDA kernel launch overhead does depend on the number of blocks, so it may not be best to launch a grid where the number of threads equals the number of input elements when the input size is very big. We'll show a pattern for dealing with large inputs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def average_sqrt_cuda_kernel(x, avg):\n",
    "    tidx = cuda.threadIdx.x # this is the unique thread ID within a 1D block\n",
    "    bidx = cuda.blockIdx.x  # Similarly, this is the unique block ID within the 1D grid\n",
    "\n",
    "    block_dimx = cuda.blockDim.x  # number of threads per block\n",
    "    grid_dimx = cuda.gridDim.x    # number of blocks in the grid\n",
    "    \n",
    "    start = tidx + bidx * block_dimx\n",
    "    stride = block_dimx * grid_dimx\n",
    "    \n",
    "    avg = 0.\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        avg += np.sqrt(x[i])\n",
    "    avg = avg/x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_per_block = 128\n",
    "blocks_per_grid = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cuda2 = %timeit -o average_sqrt_cuda_kernel([blocks_per_grid, threads_per_block](Mat, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cuda Kernel Speedup: {}\".format(time_reg.best/time_cuda2.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matrix_multiplication_cuda(A, B, C):\n",
    "    \"\"\"\n",
    "      Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tCudaMat1 = %timeit -o matrix_multiplication_cuda(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Numba 2: {}\".format(tRegMat.best/tNumMat2.best))\n",
    "print(\"Speedup CUDA 1:  {}\".format(tRegMat.best/tCudaMat1.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be a faster way for the matrix multiplication with Cuda:\n",
    "\n",
    "- Each thread block is responsible for computing a square sub-matrix of C and each thread for computing an element of the sub-matrix. \n",
    "- The sub-matrix is equal to the product of a square sub-matrix of A (sA) and a square sub-matrix of B (sB). \n",
    "- In order to fit into the device resources, the two input matrices are divided into as many square sub-matrices of dimension TPB as necessary, and the result computed as the sum of the products of these square sub-matrices.\n",
    "- Each product is performed by first loading sA and sB from global memory to shared memory, with one thread loading each element of each sub-matrix. \n",
    "- Once sA and sB have been loaded, each thread accumulates the result into a register (tmp). Once all the products have been calculated, the results are written to the matrix C in global memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float32\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matrix_multiplication_cuda_fast(A, B, C):\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tCudaMat2 = %timeit -o matrix_multiplication_cuda_fast(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Numba 2: {}\".format(tRegMat.best/tNumMat2.best))\n",
    "print(\"Speedup CUDA 1:  {}\".format(tRegMat.best/tCudaMat1.best))\n",
    "print(\"Speedup CUDA 2:  {}\".format(tRegMat.best/tCudaMat2.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandelbrot Fractal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pure Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color function for point at (x, y)\n",
    "def mandel(x, y, max_iters):\n",
    "    c = complex(x, y)\n",
    "    z = 0.0j\n",
    "    for i in range(max_iters):\n",
    "        z = z*z + c\n",
    "        if z.real*z.real + z.imag*z.imag >= 4:\n",
    "            return i\n",
    "    return max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fractal(xmin, xmax, ymin, ymax, image, iters):\n",
    "    height, width = image.shape\n",
    "\n",
    "    pixel_size_x = (xmax - xmin)/width\n",
    "    pixel_size_y = (ymax - ymin)/height\n",
    "\n",
    "    for x in range(width):\n",
    "        real = xmin + x*pixel_size_x\n",
    "        for y in range(height):\n",
    "            imag = ymin + y*pixel_size_y\n",
    "            color = mandel(real, imag, iters)\n",
    "            image[y, x]  = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gimage = np.zeros((1024, 1536), dtype=np.uint8)\n",
    "xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype('float32')\n",
    "iters = 50\n",
    "\n",
    "start = timer()\n",
    "create_fractal(xmin, xmax, ymin, ymax, gimage, iters)\n",
    "dt_py = timer() - start\n",
    "\n",
    "print(\"Mandelbrot created on CPU in {} s\".format(dt_py))\n",
    "plt.imshow(gimage);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numba Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandel_numba = jit(nb.uint32(nb.float32, nb.float32, nb.uint32))(mandel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def create_fractal_numba(xmin, xmax, ymin, ymax, image, iters):\n",
    "    height, width = image.shape\n",
    "\n",
    "    pixel_size_x = (xmax - xmin)/width\n",
    "    pixel_size_y = (ymax - ymin)/height\n",
    "\n",
    "    for x in range(width):\n",
    "        real = xmin + x*pixel_size_x\n",
    "        for y in range(height):\n",
    "            imag = ymin + y*pixel_size_y\n",
    "            color = mandel_numba(real, imag, iters)\n",
    "            image[y, x]  = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gimage = np.zeros((1024, 1536), dtype=np.uint8)\n",
    "xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype('float32')\n",
    "iters = 50\n",
    "\n",
    "start = timer()\n",
    "create_fractal_numba(xmin, xmax, ymin, ymax, gimage, iters)\n",
    "dt_numba = timer() - start\n",
    "\n",
    "print(\"Mandelbrot created on CPU in {} s\".format(dt_numba))\n",
    "plt.imshow(gimage);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandel_gpu = cuda.jit(restype=nb.uint32, \n",
    "                      argtypes=[nb.float32, nb.float32, nb.uint32], \n",
    "                      device=True)(mandel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(argtypes=[nb.float32, nb.float32, nb.float32, nb.float32, nb.uint8[:,:], nb.uint32])\n",
    "def create_fractal_kernel(xmin, xmax, ymin, ymax, image, iters):\n",
    "    height, width = image.shape\n",
    "\n",
    "    pixel_size_x = (xmax - xmin)/width\n",
    "    pixel_size_y = (ymax - ymin)/height\n",
    "\n",
    "    startX, startY = cuda.grid(2)\n",
    "    gridX = cuda.gridDim.x * cuda.blockDim.x # stride in x\n",
    "    gridY = cuda.gridDim.y * cuda.blockDim.y # stride in y\n",
    "\n",
    "    for x in range(startX, width, gridX):\n",
    "        real = xmin + x*pixel_size_x\n",
    "        for y in range(startY, height, gridY):\n",
    "            imag = ymin + y*pixel_size_y\n",
    "            color = mandel_gpu(real, imag, iters)\n",
    "            image[y, x]  = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gimage = np.zeros((1024, 1536), dtype=np.uint8)\n",
    "blockdim = (32, 8)\n",
    "griddim = (32, 16)\n",
    "xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype('float32')\n",
    "iters = 50\n",
    "\n",
    "start = timer()\n",
    "d_image = cuda.to_device(gimage)\n",
    "create_fractal_kernel[griddim, blockdim](xmin, xmax, ymin, ymax, d_image, iters)\n",
    "d_image.to_host()\n",
    "dt_cuda = timer() - start\n",
    "\n",
    "print(\"Mandelbrot created on GPU in {} s\".format(dt_cuda))\n",
    "plt.imshow(gimage);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup CPU:    {}\".format(dt_py/dt_py))\n",
    "print(\"Speedup Numba:  {}\".format(dt_py/dt_numba))\n",
    "print(\"Speedup CUDA:   {}\".format(dt_py/dt_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Things to Consider when Using Numba</font>\n",
    "\n",
    "- Numba allows its behaviour to be changed through the use of <a href=\"http://numba.pydata.org/numba-doc/latest/reference/envvars.html\">environment variables</a>. Unless otherwise mentioned, those variables have integer values and default to zero.\n",
    "- Not all the <a href=\"http://numba.pydata.org/numba-doc/latest/reference/pysupported.html\">Python features</a> are supported by Numba.\n",
    "- While Python has arbitrary-sized integers, integers in Numba-compiled functions get a fixed size through type inference (usually, the size of a machine integer). This means that arithmetic operations can wrapround or produce undefined results or overflow.\n",
    "- Numba may or may not copy global variables referenced inside a compiled function. Small global arrays are copied for potential compiler optimization with immutability assumption. However, large global arrays are not copied to conserve memory. The definition of “small” and “large” may change.\n",
    "- Numba does not work with recusive function.\n",
    "- For some operations, Numba may use a different algorithm than Python or Numpy. The results may not be bit-by-bit compatible. The difference should generally be small and within reasonable expectations. However, small accumulated differences might produce large differences at the end, especially if a divergent function is involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.1 s ± 1.7 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
