{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Supervised Machine Learning & Parallelization</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Machnine Learning Algorithms</font>\n",
    "\n",
    "Algorithms can generally be placed into one of three categories:\n",
    "\n",
    "##### Supervised learning \n",
    "    - f(X) = Y\n",
    "    - i.e. regression, classification\n",
    "    - Decision trees, Support Vector Machines, Neural Networks, ...\n",
    "    \n",
    "    \n",
    "##### Unsupervised learning \n",
    "    - f(X) = X'\n",
    "    - i.e. clustering, dimensionality reduction\n",
    "    - K-Means, Expectation Maximization, Principal component analysis, ...\n",
    "    \n",
    "##### Reinforcement learning: \n",
    "    - f(S, A, T, R) = Ï€\n",
    "    - i.e. optimal policy for markov decision processes \n",
    "    - Q-Learning, SARSA, Temporal Differencing, ...\n",
    "\n",
    "Also some which might fit in multiple e.g. Semi-supervised learning algorithms\n",
    "\n",
    "![fig_ML](https://www.pngitem.com/pimgs/m/346-3460573_types-of-machine-learning-machine-learning-types-of.png)\n",
    "Image Source: www.pngitem.com\n",
    "\n",
    "Here we will use **supervised learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Description of the Dataset</font>\n",
    "\n",
    "*Open University Learning Analytics Data (OULAD):* https://archive.ics.uci.edu/ml/datasets/Open+University+Learning+Analytics+dataset\n",
    "\n",
    "- Information about students and their interactions with a virtual learning environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assess_fname = \"https://raw.githubusercontent.com/aishwaryar7/Open-University-Learning-Analysis/master/anonymisedData/assessments.csv\"\n",
    "courses_fname = \"https://raw.githubusercontent.com/aishwaryar7/Open-University-Learning-Analysis/master/anonymisedData/courses.csv\"\n",
    "stdassess_fname = \"https://raw.githubusercontent.com/aishwaryar7/Open-University-Learning-Analysis/master/anonymisedData/studentAssessment.csv\"\n",
    "stdinfo_fname = \"https://raw.githubusercontent.com/aishwaryar7/Open-University-Learning-Analysis/master/anonymisedData/studentInfo.csv\"\n",
    "stdregi_fname = \"https://raw.githubusercontent.com/aishwaryar7/Open-University-Learning-Analysis/master/anonymisedData/studentRegistration.csv\"\n",
    "vle_fname = \"https://raw.githubusercontent.com/aishwaryar7/Open-University-Learning-Analysis/master/anonymisedData/vle.csv\"\n",
    "stdvle_fname = \"datasets/student_vle.csv\"\n",
    "list_files = [assess_fname, courses_fname, stdassess_fname, \n",
    "              stdinfo_fname, stdregi_fname, stdvle_fname, vle_fname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in list_files:\n",
    "    reader = pd.read_csv(filename)  \n",
    "    print('%s: %i samples, %i features' % (os.path.basename(filename), reader.shape[0], reader.shape[1]))\n",
    "    print('\\t','\\n\\t'.join(reader.columns),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are:\n",
    "- 32,953 student records (potentially with duplicate students)\n",
    "- 22 courses\n",
    "- 6364 VLEs (\"virtual learning environment\" - web page, essentially)\n",
    "- 10,655,280 VLE student interactions \n",
    "- 206 assessments \n",
    "- 173,912 assessment results from students\n",
    "\n",
    "**Objective:** predict a student's final result (distinction, fail, pass, withdraw)\n",
    "\n",
    "This means there are 32,953 samples we can learn from - one per student. \n",
    "So we have 11 (no id_student) features from studentInfo, and 1 (date_registration) from studentRegistration...what about others? Unclear exactly how we might represent the remaining data to an algorithm.\n",
    "\n",
    "There are also some fields that would be \"cheating\" to know a priori:\n",
    "- date_unregistration: only filled for students who have withdrawn\n",
    "- assessment scores: having all of these with the weights gives the final result itself\n",
    "- clicks on a VLE? Depends on when in the process you want to create a classifier for\n",
    "    \n",
    "In practice, the fields you can use are determined by what the algorithm has available at test time. If you want to predict student grades based on their information at registration, only the 12 previously mentioned fields can be used.\n",
    "\n",
    "Intuitively, it should be very difficult to predict a student's final result based solely on that information...but let's give it a shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(stdinfo_fname)\n",
    "display(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = pd.read_csv(stdregi_fname)\n",
    "display(regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- According to the dataset's information, **date_registration** is the number of days relative to the start of the actual course - meaning it's already preprocessed for us!\n",
    "- We now need to combine it into one set. \n",
    "- Note that the **id_student** fields are in the same order. Otherwise, we would need to go row by row to combine them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the student ids match in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(info['id_student'].equals(regr['id_student']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the date of registration to the student information dataframe to create a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = info.join(regr['date_registration'])\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data above uses text data for the majority of features. \n",
    "- For any algorithm to work, we need to convert these into numerical representations. \n",
    "\n",
    "So, best way to do that? \n",
    "\n",
    "- Simplest way is to just assign a number to each unique value in a feature. \n",
    "- However, that implies an ordering on the feature. \n",
    "- For some, e.g. **age_band**, that may make sense; but what would it mean to say **Scotland > North Western Region**? **Yorkshire Region** comes before the **South Region**? Nominal vs. Ordinal features.\n",
    "\n",
    "For these types of features - those without a natural ordering - we use a one-hot encoding vector. For example, a feature set:\n",
    "\n",
    "```python\n",
    "[Red, Green, Red, Blue]\n",
    "```\n",
    "might be changed to\n",
    "```python\n",
    "[[1 0 0]\n",
    " [0 1 0]\n",
    " [1 0 0]\n",
    " [0 0 1]]\n",
    "```\n",
    "Each column therefore represents the presence or lack of a certain attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to convert a categorical variable into dummy/indicator variables using the `get_dummies` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "code_module = pd.get_dummies(data['code_module'])\n",
    "display(code_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each column in the dataframe, print the number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data:\n",
    "    print(\"{}: {}\".format(col, data[col].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('\\n'+'\\n'.join(['%s: %i' % (col, data[col].nunique()) for col in data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ While potentially subjective, we want to label **code_module**, **code_presentation**, **gender**, **region**, and **disability** as nominal values. \n",
    "+ This gives us (7 + 4 + 2 + 13 + 1 + 1 + 1 + 1 + 1 + 2 + 1 + 1) = 35 features. \n",
    "+ We then also need to relabel the ordinal values with a logical ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal = ['highest_education', 'imd_band', 'age_band', 'final_result']\n",
    "\n",
    "for col in ordinal:\n",
    "    print(\"{}: {}\".format(col, data[col].unique()))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `?` doesn't have an ordering against the others. \n",
    "- One way to take care of it (other than classifying as nominal) is to drop those 1111 samples. \n",
    "- Another option is to label as NaNif the chosen ML algorithm can handle these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    if '?' in set(data[col]):\n",
    "        print('Column %s has %s unknown values' % (col, data[col].value_counts()['?']))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new dataframe where new columns for nominal and ordinal values are added.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    numeric_keys = ['num_of_prev_attempts', 'studied_credits', 'date_registration']\n",
    "    nominal_keys = ['code_module', 'code_presentation', 'region', 'disability', 'gender']\n",
    "    ordinal_keys = {'highest_education': ['No Formal quals', 'Lower Than A Level', 'A Level or Equivalent', \n",
    "                                          'HE Qualification', 'Post Graduate Qualification'],\n",
    "                   'imd_band': ['0-10%', '10-20', '20-30%', '30-40%', '40-50%', '50-60%',\n",
    "                                '60-70%', '70-80%', '80-90%', '90-100%'],\n",
    "                   'age_band': ['0-35', '35-55', '55<='],\n",
    "    #                 'gender':['M', 'F'],\n",
    "                   'final_result': ['Fail', 'Withdrawn', 'Pass', 'Distinction']}\n",
    "    \n",
    "    # Get rid of unknown values\n",
    "    data = data.drop('id_student', axis=1).replace({'?':np.nan})\n",
    "\n",
    "    all_keys = numeric_keys + nominal_keys + list(ordinal_keys.keys()) \n",
    "    remaining_keys = set(data.columns) - set(all_keys)\n",
    "    numeric_keys += list(remaining_keys)\n",
    "    \n",
    "    def to_ord(x, order):\n",
    "        \"\"\"\n",
    "           Create a categorical data type. \n",
    "           https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n",
    "        \"\"\"\n",
    "        y = pd.Categorical(x, categories=order)\n",
    "        return y.codes\n",
    "\n",
    "    # Numeric columns\n",
    "    numeric = data[numeric_keys]\n",
    "\n",
    "    # One-hot encoded nominal columns\n",
    "    nominal = pd.get_dummies(data[nominal_keys])\n",
    "\n",
    "    # Set the ordinal columns as a 'category' type and retrieve the resulting conversion\n",
    "    # Can only do this with series, so have to perform column-by-column and then reform \n",
    "    # into a dataframe\n",
    "    ordinal = np.array([to_ord(data[o], ordinal_keys[o]) for o in sorted(ordinal_keys)])\n",
    "    ordinal = pd.DataFrame(ordinal.T, columns=sorted(ordinal_keys))\n",
    "\n",
    "    # Join the dataframes together, drop nan values, convert to integers, and reset the index back to default\n",
    "    final_data = numeric.join(nominal).join(ordinal).dropna().astype(np.int32).reset_index(drop=True)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = clean_data(data)\n",
    "display(final_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Choosing an Algorithm</font>\n",
    "\n",
    "*Algorithm choice without background knowledge is arbitrary.*\n",
    "\n",
    "\"No Free Lunch\" theorem:\n",
    "\n",
    "> \"We have dubbed the associated results NFL theorems because they demonstrate that if an algorithm performs well on a certain class of problems then it necessarily pays for that with degraded performance on the set of all remaining problems.\" - Wolpert & Macready (1997)\n",
    "    \n",
    "Luckily, there is always background knowledge. We can *assume* a great deal even without knowing this dataset:\n",
    "- Samples that have similar features are usually going to be similarly classified\n",
    "- There will likely be outliers in the data\n",
    "- There is going to be little or no noise in our current feature set\n",
    "- There is not any timeseries component or ordering in time to these features\n",
    "- Many other assumptions we make by the sheer fact of having this dataset. \n",
    "\n",
    "These remove choices like reinforcement learning algorithms, NLP algorithms, forecasting algorithms, etc. \n",
    "\n",
    "There is also the background knowledge of what you would like to receive from the algorithm: \n",
    "- interpretable model?\n",
    "- feature importances?\n",
    "- exact boundary conditions?\n",
    "- exact function approximation?\n",
    "- speed?\n",
    "\n",
    "All of these contribute to the choice of algorithm. Here, we'll use **decision trees** - they essentially provide four of the five above conditions, and are fairly powerful for such a simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">(binary) Decision Trees</font>\n",
    "\n",
    "Quick summary:\n",
    "- Tree is built by selecting a feature to 'split' on at each node; all samples with the feature value go to one branch, and all other samples go to the other\n",
    "- Few different splitting metrics, but Gini and Information Gain are the most common. General idea is to determine which feature choice gives the greatest separation between target classes\n",
    "- Adaptable to continuous data as well as regression settings\n",
    "\n",
    "Simple decision tree for passenger survival on the Titanic:\n",
    "\n",
    "![fig_tree](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)\n",
    "Image Source: wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "def split_data(X, Y, pct=0.2):\n",
    "    ''' \n",
    "       Divide data into training / validation set \n",
    "    '''\n",
    "    X = X.as_matrix()\n",
    "    samples = X.shape[0]\n",
    "    n_train = int(samples * pct) \n",
    "    indices = np.arange(samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Training set\n",
    "    Xt = X[indices[:n_train]]\n",
    "    Yt = Y[indices[:n_train]]\n",
    "\n",
    "    # Validation set\n",
    "    Xv = X[indices[n_train:]]\n",
    "    Yv = Y[indices[n_train:]]\n",
    "    return (Xt, Yt), (Xv, Yv)\n",
    "\n",
    "\n",
    "def fit_data(X, Y, kwargs={}, Model=DecisionTreeClassifier):\n",
    "    '''\n",
    "    Fit model and test performance on a validation set\n",
    "    \n",
    "    kwargs:\n",
    "     - max_depth\n",
    "     - max_leaf_nodes\n",
    "     - min_impurity_split\n",
    "     - min_samples_split\n",
    "     - min_samples_leaf\n",
    "     - criterion\n",
    "     - class_weight\n",
    "    '''\n",
    "    training, (Xv, Yv) = split_data(X, Y)\n",
    "\n",
    "    model = Model(**kwargs).fit(*training)\n",
    "    Y_hat = model.predict(Xv)\n",
    "\n",
    "    print('Accuracy:', accuracy_score(Yv, Y_hat))\n",
    "    print('F1:', f1_score(Yv, Y_hat, average='weighted'))\n",
    "    print('Confusion Matrix:\\n', confusion_matrix(Yv, Y_hat))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = final_data['final_result']\n",
    "X = final_data.drop('final_result', axis=1)\n",
    "\n",
    "fit_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so great. So, what do we do now?\n",
    "\n",
    "A few options:\n",
    " - Change the objective\n",
    " - Improve model\n",
    " - Improve data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = final_data['final_result'].replace({1:0, 2:1, 3:1}) # Fail, Withdraw, Pass, Distinction -> Incomplete, Complete\n",
    "X = final_data.drop('final_result', axis=1)\n",
    "\n",
    "fit_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement, but still not much better than a coin flip. How about improving the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import time\n",
    "\n",
    "# Create a timing decorator \n",
    "def timer(f):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start  = time()\n",
    "        result = f(*args, **kwargs)\n",
    "        print('Execution time: %.2f seconds' % (time() - start))\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# Split data using sklearn\n",
    "Xt, Xv, Yt, Yv = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "@timer\n",
    "def run_search_dt(n_jobs=1):\n",
    "    search = GridSearchCV(estimator = DecisionTreeClassifier(),\n",
    "                          param_grid= {'max_depth'      : np.arange(2, 11),\n",
    "                                       'max_leaf_nodes' : np.linspace(8, 100, 10, dtype=np.int32),\n",
    "                                       'criterion'      : ['gini', 'entropy'],\n",
    "                                       'class_weight'   : ['balanced', None]},\n",
    "                          scoring = make_scorer(f1_score),\n",
    "                          n_jobs  = n_jobs,\n",
    "                          cv      = 3) # 3 fold cross validation\n",
    "    search.fit(Xt, Yt)\n",
    "    display(pd.DataFrame(search.cv_results_))\n",
    "    print('Best score: %.3f' % search.best_score_)\n",
    "    return search.best_params_\n",
    "    \n",
    "    \n",
    "params = run_search_dt()\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = run_search_dt(4)\n",
    "fit_data(X, Y, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Ensembles </font>\n",
    "\n",
    "Ensemble learning is the idea that we can potentially combine multiple models into one bigger model, which on average outputs better estimates than any of its constituents. \n",
    "\n",
    "Number of different approaches:\n",
    " - Bayesian methods\n",
    " - Stacking\n",
    " - Bagging\n",
    " - Boosting\n",
    " - ...\n",
    " \n",
    "Longer training times, but typically ensembles using even extremely simple models (e.g. single split decision tree \"stumps\") can outperform more specialized models. \n",
    "\n",
    "\n",
    "### Boosting\n",
    "\n",
    "Given a weak learner i.e. performing consistently better than random, are we able to make it a strong learner?\n",
    "\n",
    "Yes: \n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "Assume you've trained the weak learner and have its outputs on the training set. These outputs match well with some examples, and poorly with others. The poor output samples can be reweighted to have a higher importance in the training set, and then another training round for a weak learner is performed. \n",
    "\n",
    "Each weak learner has an associated weight based on its overall sample-weighted performance. These are used after all boosting rounds are completed to combined the weak learners into a single ensemble model. \n",
    "\n",
    "\n",
    "#### Gradient Boosting\n",
    "\n",
    "Continuing with that idea: rather than reweight the samples and refit them at each iteration, we can instead fit the residual at each step - the negative gradient of the squared error:\n",
    "\n",
    "$F_{t+1}(x) = \\dfrac{\\delta L(y, F_t(x))}{\\delta F_t(x)} = y - F_t(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost.sklearn  import XGBClassifier\n",
    "\n",
    "timed_test = timer(fit_data)\n",
    "\n",
    "for M in [AdaBoostClassifier, XGBClassifier]:\n",
    "    print(M.__name__)\n",
    "    timed_test(X, Y, Model=M)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> Feature Importance</font>\n",
    "\n",
    "- One benefit to using tree-based methods is that they are white-boxes: their models are easily explainable. \n",
    "- This leads to easy calculation of things like feature importance and decision paths.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fit_data(X, Y, Model=XGBClassifier)\n",
    "importance = sorted(list(zip(X.columns, model.feature_importances_)), key=lambda k:k[1], reverse=True)\n",
    "print('Top three features:')\n",
    "display(importance[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "imp = np.array(importance)\n",
    "imp = pd.DataFrame(imp[:,1], index=imp[:,0]).T\n",
    "\n",
    "plt.figure(figsize=(10,10));\n",
    "sns.barplot(data=imp, orient='h', palette='RdBu').set_xlabel('Relative Importance %');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.plotting import plot_tree\n",
    "plt.figure(figsize=(14,13));\n",
    "plot_tree(model, ax=plt.gca());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Learning Curves </font>\n",
    "\n",
    "Another thing we can look at is the learning curves, to judge a few different things. \n",
    "\n",
    "1. Determine if more data would improve the results\n",
    "2. Check if our model is overfitting\n",
    "3. Bias / variance examination\n",
    "\n",
    "\n",
    "#### Bias-variance tradeoff \n",
    "\n",
    "Generally speaking:\n",
    " - Bias: error on the data; high bias = underfitting\n",
    " - Variance: sensitivity to small differences between samples; high variance = overfitting\n",
    " \n",
    "Supervised learning models always have an inherent bias-variance tradeoff in their construction. Usually this is tweakable via the hyperparameters of the model, such as with the maximum depth of a decision tree.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve, ShuffleSplit\n",
    "\n",
    "def plot_learning_curve(X, Y, Model=XGBClassifier):\n",
    "    cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "    train_sizes = np.linspace(0.1,1,8)\n",
    "    train_sizes, train_scores, test_scores = learning_curve(Model(), \n",
    "                                                            X, Y, cv=cv, \n",
    "                                                            train_sizes=train_sizes, n_jobs=4)\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std  = np.std(train_scores, axis=1)\n",
    "    test_mean  = np.mean(test_scores, axis=1)\n",
    "    test_std   = np.std(test_scores, axis=1)\n",
    "\n",
    "    c1 = plt.plot(train_sizes, train_mean, 'o-', label=\"Training score\")[0].get_color()\n",
    "    c2 = plt.plot(train_sizes, test_mean, 'o-', label=\"Cross-validation score\")[0].get_color()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_mean-train_std, train_mean+train_std, alpha=0.1, color=c1)\n",
    "    plt.fill_between(train_sizes, test_mean-test_std, test_mean+test_std, alpha=0.1, color=c2)\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "plot_learning_curve(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than that, feature engineering would be the next step:\n",
    " - group of students in the same code module?\n",
    " - student in multiple code modules?\n",
    " - student performance in prior modules?\n",
    " - etc. \n",
    " \n",
    "Let's take a look at the data again however - maybe changing our objective to a point-in-time estimate will get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vle = pd.read_csv(stdvle_fname)\n",
    "display(vle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vle['date'].hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = vle.groupby(['code_module', 'code_presentation', 'id_student', 'date'])['sum_click'].sum().reset_index()\n",
    "display(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.groupby(['date'])['sum_click'].sum().reset_index().plot(x='date', y='sum_click');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_clicks(data, day):\n",
    "    # Get all data points on this day or prior, and sum them over the three columns we care about\n",
    "    clicks = s[s['date'] <= day].groupby(['code_module', 'code_presentation', 'id_student'])['sum_click'].sum().reset_index()\n",
    "    \n",
    "    # Merge this data with the appropriate rows in the full data set\n",
    "    full   = pd.merge(data, clicks, on=['code_module', 'code_presentation', 'id_student'], how='outer')\n",
    "    \n",
    "    # Replace any missing values with 0\n",
    "    full['sum_click'].fillna(0, inplace=True)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Multiprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "assert(0), 'Kernel crash on windows'\n",
    "\n",
    "# Multiprocessing can significantly speed up execution, and isn't too difficult to implement\n",
    "# Define a function you want mapped to each sample in the data, and then apply it\n",
    "def test_day(day):\n",
    "    wclicks = add_clicks(data)\n",
    "    cleaned = clean_data(wclicks)\n",
    "    \n",
    "    Y = cleaned['final_result'].replace({1:0, 2:1, 3:1}) # Fail, Withdraw, Pass, Distinction -> Incomplete, Complete\n",
    "    X = cleaned.drop('final_result', axis=1)\n",
    "    Xt, Xv, Yt, Yv = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    model = XGBClassifier().fit(Xt, Yt)\n",
    "    Y_hat = model.predict(Xv)\n",
    "    return day, f1_score(Yv, Y_hat)\n",
    "\n",
    "pool = Pool(4)\n",
    "xy_performance = pool.map(test_day, sorted(s['date'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If Multiprocessing is not Available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# If multiprocessing isn't available, can still do the same work normally\n",
    "xy_performance = []\n",
    "for day in tqdm(sorted(s['date'].unique())):\n",
    "    wclicks = add_clicks(data, day)\n",
    "    cleaned = clean_data(wclicks)\n",
    "    \n",
    "    Y = cleaned['final_result'].replace({1:0, 2:1, 3:1}) # Fail, Withdraw, Pass, Distinction -> Incomplete, Complete\n",
    "    X = cleaned.drop('final_result', axis=1)\n",
    "    Xt, Xv, Yt, Yv = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    model = XGBClassifier().fit(Xt, Yt)\n",
    "    Y_hat = model.predict(Xv)\n",
    "    xy_performance.append([day, f1_score(Yv, Y_hat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_performance = np.array(sorted(xy_performance, key=lambda k:k[0]))\n",
    "plt.plot(*xy_performance.T);\n",
    "plt.xlabel('Days Relative to Start of Course');\n",
    "plt.ylabel('F1 Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with just adding information about the number of clicks a student has made, we can improve the score significantly. Since we're looking at data going forward anyway, we can also add test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_scores = pd.read_csv(stdassess_fname)\n",
    "display(student_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info = pd.read_csv(assess_fname)\n",
    "display(test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test = pd.merge(student_scores, test_info, on=['id_assessment'], how='outer')\n",
    "combined_test = combined_test.replace({'?':np.nan}).dropna()\n",
    "display(combined_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = combined_test.drop(['assessment_type', 'is_banked', 'id_assessment'], axis=1)\n",
    "\n",
    "# Create a few new features, combining other columns together\n",
    "c['relative_turnin'] = c['date'].astype(float) - c['date_submitted'].astype(float)\n",
    "c['weighted_score']  = c['score'].astype(float) * c['weight'] / 100.\n",
    "c['raw_score'] = c['score'].astype(float) # Modify type\n",
    "display(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scores(data, day, key='date_submitted'):    \n",
    "    # Same as before - get the relevant data and group by necessary columns\n",
    "    assess = c[c['date_submitted'] <= day].groupby(['code_module', 'code_presentation', 'id_student'])\n",
    "\n",
    "    # Sum over the relevant features\n",
    "    total  = assess['raw_score', 'relative_turnin', 'weighted_score'].sum().reset_index()\n",
    "\n",
    "    # Merge with the rest of the data\n",
    "    full   = pd.merge(data, total, on=['code_module', 'code_presentation', 'id_student'], how='outer')\n",
    "\n",
    "    # Replace missing values\n",
    "    full['relative_turnin'].fillna(0, inplace=True)\n",
    "    full['weighted_score'].fillna(0, inplace=True)\n",
    "    full['raw_score'].fillna(0, inplace=True)\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_performance = []\n",
    "for day in tqdm(sorted(s['date'].unique())):\n",
    "    wclicks = add_clicks(data, day)\n",
    "    wscores = add_scores(wclicks, day)\n",
    "    cleaned = clean_data(wscores)\n",
    "    \n",
    "    Y = cleaned['final_result'].replace({1:0, 2:1, 3:1}) # Fail, Withdraw, Pass, Distinction -> Incomplete, Complete\n",
    "    X = cleaned.drop('final_result', axis=1)\n",
    "    Xt, Xv, Yt, Yv = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    model = XGBClassifier().fit(Xt, Yt)\n",
    "    Y_hat = model.predict(Xv)\n",
    "    xy_performance.append([day, f1_score(Yv, Y_hat)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_performance = np.array(sorted(xy_performance, key=lambda k:k[0]))\n",
    "plt.plot(*xy_performance.T);\n",
    "plt.xlabel('Days Relative to Start of Course');\n",
    "plt.ylabel('F1 Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, not bad: we can predict with ~80% accuracy whether a student will complete the course after only about a month and a half - 20% of the way through. <br><br>Even at the first day of the course, we're at nearly 70%. Let's take a look at how each of the original objectives fare over time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from xgboost.sklearn import XGBClassifier\n",
    "xy_performance = []\n",
    "for day in tqdm(sorted(s['date'].unique())):\n",
    "    wclicks = add_clicks(data, day)\n",
    "    wscores = add_scores(wclicks, day)\n",
    "    cleaned = clean_data(wscores)\n",
    "    \n",
    "    Y = cleaned['final_result']\n",
    "    X = cleaned.drop('final_result', axis=1)\n",
    "    Xt, Xv, Yt, Yv = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "    model = XGBClassifier().fit(Xt, Yt)\n",
    "    Y_hat = model.predict(Xv)\n",
    "    xy_performance.append([day,] + list(f1_score(Yv, Y_hat, average=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = np.array(sorted(xy_performance, key=lambda k:k[0]))\n",
    "XY = pd.DataFrame(XY, columns=['Days', 'Fail', 'Withdraw', 'Pass', 'Distinction'])\n",
    "display(XY)\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "XY.plot(x='Days').set_ylabel('F1 Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning is an iterative process over two steps:\n",
    " - Improve the data\n",
    " - Improve the model\n",
    " \n",
    "When possible, improving the objective can also help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Advanced Topics</font>\n",
    "\n",
    "- **Translation**. Not necessarily just language to language, but in general data to data where the two sets have a (nearly) one-to-one mapping. Autoencoders are popular for this right now, e.g. Seq2Seq\n",
    "<br><br>\n",
    "- **Deep NLP**. Any time you want to use text as the feature in a neural network, word vector embeddings are the go to representation e.g. GloVe. Long short-term memory units are the building blocks for most models, and recurrent networks in general tend to perform fairly well.\n",
    "<br><br>\n",
    "- **Temporal data**. Recurrent networks are the standard model when trying to extract temporal relationships within a data set. Recurrent networks can also be used for a number of other cool applications: https://distill.pub/2016/augmented-rnns/ . Aside from those, reservoir computing networks (Echo State Networks & Liquid State Machines) are some lesser known but powerful models (and also my favorite topics).\n",
    "<br><br>\n",
    "- **Small data set**. The general notion is that you need a _lot_ of data to perform deep learning. While it helps, it's not necessarily a requirement - Generative Adversarial Networks can help to create a simulated data set which is 'good enough' for getting a training process started. Utilizing these correctly can take some work however. \n",
    "<br><br>\n",
    "- **Image to image**. Also GAN domain, though depending on the specific application convolutional networks are useful as well (and are actually the building blocks for GANs. \n",
    "<br><br>\n",
    "- **Markov Processes**. Anything that can be imagined as taking actions in a state to reach a goal (or optimize the state) are typically the domain of reinforcement learning. The standard research environment (or most visible at least) is in game playing: there's an obvious state transition function, action mapping, and reward sequence to train a model. Plus, there's no shortage of environments.\n",
    "<br><br>\n",
    "- **Different data domains**. If you have say, a _large_ set of simulated data or data which is particular to a specific domain, but you want to create a model which can learn a _related_ domain. There is a lot of research on transfer learning happening right now, where models trained on one domain can transfer knowledge of what they learned to be used in the separate (but somehow related) domain. \n",
    "<br><br>\n",
    "- Tons of other models for nearly any problem you can think of. Any which I haven't mentioned?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
