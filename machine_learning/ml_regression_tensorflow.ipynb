{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg)\n",
    "\n",
    "<center>\n",
    "<h1><font size=\"+3\">NCCS Training Course Series</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h2><font color=\"red\">Machine Learning Regression Model with Tensorflow</font></h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Reference\n",
    "\n",
    "- <a href=\"https://www.mygreatlearning.com/blog/what-is-tensorflow-machine-learning-library-explained/\">What is TensorFlow? The Machine Learning Library Explained</a>\n",
    "- <a href=\"https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial\">Getting Started with TensorFlow: A Machine Learning Tutorial</a>\n",
    "- <a href=\"https://stackabuse.com/tensorflow-2-0-solving-classification-and-regression-problems/\">Tensorflow 2.0: Solving Classification and Regression Problems</a>\n",
    "- <a href=\"https://sebastianraschka.com/faq/docs/tensorflow-vs-scikitlearn.html\">What is the main difference between TensorFlow and scikit-learn?</a>\n",
    "\n",
    "## <font color=\"red\">What is TensorFlow?</font>\n",
    "- Tensorflow is an open-source library for numerical computation and large-scale machine learning that ease `Google Brain TensorFlow`, the process of acquiring data, training models, serving predictions, and refining future results.\n",
    "- Tensorflow bundles together Machine Learning and Deep Learning models and algorithms.\n",
    "- Tensorflow allows developers to create a graph of computations to perform. Each node in the graph represents a mathematical operation and each connection represents data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Steps of a ML Program\n",
    "    \n",
    "![FIG_AXES](https://d1m75rqqgidzqn.cloudfront.net/2019/10/what-is-machine-learning-7-steps-of-machine-learning.jpg)\n",
    "Image Source: Vaishali Advani (SimpliLearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Problem Statement</font>\n",
    "\n",
    "We consider the function: <br>\n",
    "$$\n",
    "f(x,y) = (1-(x^2 + y^3))e^{-\\frac{1}{2}(x^2 + y^2)}\n",
    "$$\n",
    "<br>\n",
    "defined in the domain $D=[-3,3] \\times [-3,3]$.\n",
    "<OL>\n",
    "<LI> We randomnly select $n$ points in the domain $D$ and compute the function on those points to create a dataset containing the pairs points/values.\n",
    "<LI> We use the dataset for training a ML algorithm.\n",
    "<LI> We generate a uniform set of points in $D$ to test the algorithm.\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Generating the Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(x,y):\n",
    "    return (1-(x**2+y**3))*np.exp(-(x**2+y**2)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = 2\n",
    "nx = 30\n",
    "ny = 30\n",
    "num_points = nx * ny\n",
    "\n",
    "# Boundary of the domain\n",
    "a_min = -3.0\n",
    "a_max = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Generate dataset for training</font>\n",
    "- The grid points are randomly generated over the domain\n",
    "- The arrays are 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = np.zeros(num_points)  # 1D targets for training\n",
    "xt = np.zeros((num_points, num_dims))  # grid points for training\n",
    "\n",
    "x = np.random.uniform(a_min, a_max, nx) # Feature vectors\n",
    "y = np.random.uniform(a_min, a_max, ny) # Labels\n",
    "\n",
    "k = 0\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        xt[k, 0] = x[i]\n",
    "        xt[k, 1] = y[j]\n",
    "        yt[k] = ff(x[i], y[j])\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Add noise in the training targets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian normal distribution with noise_mean as mean\n",
    "# and noise_std as standard deviation\n",
    "noise_mean = 0.0\n",
    "noise_std  = 1.0e-2\n",
    "noise = np.random.normal(noise_mean, noise_std, num_points)\n",
    "yt = yt + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Generate dataset for validation</font>\n",
    "- The grid points are uniformly distributed over the domain\n",
    "- The arrays are 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yv = np.zeros(num_points)  # 1D targets for validation\n",
    "xv = np.zeros((num_points, num_dims))  # grid points for validation\n",
    "\n",
    "x = np.linspace(-3.0, 3.0, nx)\n",
    "y = np.linspace(-3.0, 3.0, ny)\n",
    "\n",
    "k = 0\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        xv[k,0] = x[i]\n",
    "        xv[k,1] = y[j]\n",
    "        yv[k] = ff(x[i],y[j])\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Data Gathering and Basic Analyses</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({\"x0\": xt[:,0], \"x1\": xt[:,1], \"TargetValues\": yt[:]})\n",
    "print(train_data.head(5))                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data to be used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data  = pd.DataFrame({\"x0\": xv[:,0], \"x1\": xv[:,1], \"TargetValues\": yv[:]})\n",
    "print(valid_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure().gca(projection='3d');\n",
    "threedee.scatter(train_data['x0'], train_data['x1'], train_data['TargetValues']);\n",
    "threedee.set_xlabel('x');\n",
    "threedee.set_ylabel('y');\n",
    "threedee.set_zlabel('f(x,y)');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the joint distribution of the columns from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_data.drop(columns=[\"TargetValues\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do something similar for the data used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(valid_data.drop(columns=[\"TargetValues\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the overall statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train_data.describe()\n",
    "train_stats.pop(\"TargetValues\")\n",
    "train_stats = train_stats.transpose()\n",
    "print(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split features from labels\n",
    "- Separate the target value, or `label`, from the features.\n",
    "- This `label` is the value that you will train the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_data.pop('TargetValues')\n",
    "valid_labels = valid_data.pop('TargetValues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Normailized the Data</font>\n",
    "\n",
    "- It is good practice to normalize features that use different scales and ranges. \n",
    "- Although the model might converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x):\n",
    "  return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "# This normalized data is what we will use to train the model.\n",
    "normed_train_data = normalize_data(train_data)\n",
    "normed_valid_data = normalize_data(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Build the Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate a sequential model using `keras`\n",
    "- The sequential model is the simplest model to use, especially when getting started.\n",
    "- It involves defining a Sequential class and adding layers to the model one by one in a linear manner, from input to output.\n",
    "- The model needs to know what input shape (`input_shape`) it should expect. The first layer of the `Sequential` model needs to receive the information.\n",
    "\n",
    "In the model below:\n",
    "\n",
    "- The model expects rows of data with num_shape variables (the input_shape=num_shape argument)\n",
    "- The first hidden layer has 64 nodes and uses the `relu` activation function.\n",
    "- The second hidden layer has 64 nodes and uses the `relu` activation function.\n",
    "- The output layer has one node and uses no activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shape = len(train_data.keys())\n",
    "num_nodes = 64\n",
    "\n",
    "model = keras.Sequential([\n",
    "             layers.Dense(num_nodes, activation=tf.nn.relu, input_shape=[num_shape]),\n",
    "             layers.Dense(num_nodes, activation=tf.nn.relu),\n",
    "             layers.Dense(1) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model\n",
    "Compiling the model uses the efficient numerical libraries (Theano or TensorFlow) in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required to provide a loss function and an optimizer\n",
    "model.compile(loss = 'mse',\n",
    "              optimizer = optimizer,\n",
    "              metrics = ['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 samples from the training data and call model.predict\n",
    "example_batch = normed_train_data[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "print(example_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Train the Model</font>\n",
    "\n",
    "Training occurs over epochs and each epoch is split into batches.\n",
    "\n",
    "- **Epoch**: One pass through all of the rows in the training dataset.\n",
    "- **Batch**: One or more samples considered by the model within an epoch before weights are updated.\n",
    "- One epoch is comprised of one or more batches, based on the chosen batch size and the model is fit for many epochs. \n",
    "\n",
    "The model is \"fit\" to the training data using the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 1000 epochs, and record the training and \n",
    "# validation accuracy in the history object\n",
    "\n",
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs):\n",
    "          if epoch % 100 == 0: \n",
    "             print('')\n",
    "          print('.', end='')\n",
    "\n",
    "# How many times we go through the entire dataset\n",
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(normed_train_data, train_labels,    \n",
    "                    epochs=EPOCHS, verbose=1, callbacks=[PrintDot()])\n",
    "#epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the model's training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the stats stored in the history object.\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Target]')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "             label='Train Error')\n",
    "#    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "#             label = 'Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([min(hist['mean_absolute_error']) ,max(hist['mean_absolute_error'])])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$Target^2$]')\n",
    "    plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "             label='Train Error')\n",
    "#    plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "#             label = 'Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,max(hist['mean_squared_error'])])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(normed_valid_data, valid_labels, verbose=1)\n",
    "#print(\"Testing set Mean Abs Error: {} \".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Make Prediction</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = model.predict(normed_valid_data).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the 45-degree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(valid_labels, valid_predictions);\n",
    "plt.xlabel('True Values');\n",
    "plt.ylabel('Predictions');\n",
    "plt.axis('equal');\n",
    "plt.axis('square');\n",
    "plt.xlim([0,plt.xlim()[1]]);\n",
    "plt.ylim([0,plt.ylim()[1]]);\n",
    "_ = plt.plot([-100, 100], [-100, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = valid_predictions - valid_labels\n",
    "plt.hist(error, bins = 25);\n",
    "plt.xlabel(\"Prediction Error\");\n",
    "_ = plt.ylabel(\"Count\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure().gca(projection='3d');\n",
    "threedee.scatter(valid_data['x0'], valid_data['x1'], valid_predictions);\n",
    "threedee.set_xlabel('x');\n",
    "threedee.set_ylabel('y');\n",
    "threedee.set_zlabel('f(x,y)');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Exercise</font>\n",
    "\n",
    "Consider the 2D problem presented here.\n",
    "- Create a dataset of 1000 randomly selected points (in the domain) and their associated targets.\n",
    "- Randomly choose 80% of the data for training and the remaining for testing\n",
    "- Create your ML model and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
