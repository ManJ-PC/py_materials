{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg)\n",
    "\n",
    "<center>\n",
    "<h1><font size=\"+3\">NCCS Training Course Series</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h2><font color=\"red\">Machine Learning Regression Model with Tensorflow</font></h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Reference\n",
    "\n",
    "- <a href=\"https://www.mygreatlearning.com/blog/what-is-tensorflow-machine-learning-library-explained/\">What is TensorFlow? The Machine Learning Library Explained</a>\n",
    "- <a href=\"https://www.tensorflow.org/tutorials/keras/regression\">Basic regression: Predict fuel efficiency</a>\n",
    "- <a href=\"https://stackabuse.com/tensorflow-2-0-solving-classification-and-regression-problems/\">Tensorflow 2.0: Solving Classification and Regression Problems</a>\n",
    "- <a href=\"https://www.toptal.com/machine-learning/tensorflow-machine-learning-tutorial\">Getting Started with TensorFlow: A Machine Learning Tutorial</a>\n",
    "- <a href=\"https://sebastianraschka.com/faq/docs/tensorflow-vs-scikitlearn.html\">What is the main difference between TensorFlow and scikit-learn?</a>\n",
    "- <a href=\"https://adventuresinmachinelearning.com/python-tensorflow-tutorial/\">Python TensorFlow Tutorial – Build a Neural Network</a>\n",
    "\n",
    "## <font color=\"red\">What is TensorFlow?</font>\n",
    "- Tensorflow is an open-source library for numerical computation and large-scale machine learning that ease `Google Brain TensorFlow`, the process of acquiring data, training models, serving predictions, and refining future results.\n",
    "- Tensorflow bundles together Machine Learning and Deep Learning models and algorithms.\n",
    "- Tensorflow allows developers to create a graph of computations to perform. Nodes in the graph represent mathematical operations and connections (edges) represent data which usually are multidimensional data arrays or tensors, that are communicated between these edges.\n",
    "- The name `TensorFlow` is derived from the operations which neural networks perform on multidimensional data arrays or tensors! It’s literally a flow of tensors.\n",
    "\n",
    "\n",
    "**First Example of TensorFlow Graph**\n",
    "\n",
    "Consider the expression:\n",
    "<center>\n",
    "    a = (b + c) * (c + 2)\n",
    "</center>\n",
    "We can break this down into:\n",
    "<center>\n",
    "    d = b + c\n",
    "    \n",
    "    e = c + 2\n",
    "    \n",
    "    a = d * e\n",
    "</center>\n",
    "Now we can represent these operations graphically as:\n",
    "\n",
    "![fig_gr1](https://i1.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2017/03/Simple-graph-example.png)\n",
    "Image Source: adventuresinmachinelearning.com\n",
    "\n",
    "Note that the operations `d = b + c` and `e = c + 2` can be performed in parallel: potential of distributing such calcultions across CPUs and GPUs. \n",
    "\n",
    "**Second Example of TensorFlow Graph**\n",
    "\n",
    "The graph below shows the computational graph of a three-layer neural network.\n",
    "The animated data flows between different nodes in the graph are tensors which are multi-dimensional data arrays. \n",
    "\n",
    "![fig_gr2](https://i1.wp.com/adventuresinmachinelearning.com/wp-content/uploads/2017/03/TensorFlow-data-flow-graph.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Steps of a ML Program\n",
    "    \n",
    "![FIG_AXES](https://www.altudo.co/-/media/altudo/images/resources/blogs/5-steps-to-define-ml-flow-to-deliver-custom-user-experience/2.ashx?la=en&hash=0A8E8BEC05A4C64C37908FB87757285E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Problem Statement</font>\n",
    "\n",
    "We consider the function: <br>\n",
    "$$\n",
    "f(x,y) = (1-(x^2 + y^3))e^{-\\frac{1}{2}(x^2 + y^2)}\n",
    "$$\n",
    "<br>\n",
    "defined in the domain $D=[-3,3] \\times [-3,3]$.\n",
    "<OL>\n",
    "<LI> We randomnly select $n$ points in the domain $D$ and compute the function on those points to create a dataset containing the pairs points/values.\n",
    "<LI> We use the dataset for training a ML algorithm.\n",
    "<LI> We generate a uniform set of points in $D$ to test the algorithm.\n",
    "</OL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Generating the Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff(x,y):\n",
    "    return (1-(x**2+y**3))*np.exp(-(x**2+y**2)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = 2\n",
    "nx = 30\n",
    "ny = 30\n",
    "num_points = nx * ny\n",
    "\n",
    "# Boundary of the domain\n",
    "a_min = -3.0\n",
    "a_max = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Generate dataset for training</font>\n",
    "- The grid points are randomly generated over the domain\n",
    "- The arrays are 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt = np.zeros(num_points)  # 1D targets for training\n",
    "xt = np.zeros((num_points, num_dims))  # grid points for training\n",
    "\n",
    "x = np.random.uniform(a_min, a_max, nx) # Feature vectors\n",
    "y = np.random.uniform(a_min, a_max, ny) # Labels\n",
    "\n",
    "k = 0\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        xt[k, 0] = x[i]\n",
    "        xt[k, 1] = y[j]\n",
    "        yt[k] = ff(x[i], y[j])\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Add noise in the training targets</font>\n",
    "\n",
    "Gaussian normal distribution with `noise_mean` as mean and `noise_std` as standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_mean = 0.0\n",
    "noise_std  = 1.0e-2\n",
    "noise = np.random.normal(noise_mean, noise_std, num_points)\n",
    "yt = yt + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Generate dataset for validation</font>\n",
    "- The grid points are uniformly distributed over the domain\n",
    "- The arrays are 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yv = np.zeros(num_points)  # 1D targets for validation\n",
    "xv = np.zeros((num_points, num_dims))  # grid points for validation\n",
    "\n",
    "x = np.linspace(-3.0, 3.0, nx)\n",
    "y = np.linspace(-3.0, 3.0, ny)\n",
    "\n",
    "k = 0\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        xv[k,0] = x[i]\n",
    "        xv[k,1] = y[j]\n",
    "        yv[k] = ff(x[i],y[j])\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Data Gathering and Basic Analyses</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({\"x0\": xt[:,0], \"x1\": xt[:,1], \"TargetValues\": yt[:]})\n",
    "print(train_data.head(5))                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data to be used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data  = pd.DataFrame({\"x0\": xv[:,0], \"x1\": xv[:,1], \"TargetValues\": yv[:]})\n",
    "print(valid_data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure().gca(projection='3d');\n",
    "threedee.scatter(train_data['x0'], train_data['x1'], train_data['TargetValues']);\n",
    "threedee.set_xlabel('x');\n",
    "threedee.set_ylabel('y');\n",
    "threedee.set_zlabel('f(x,y)');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the joint distribution of the columns from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_data.drop(columns=[\"TargetValues\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do something similar for the data used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(valid_data.drop(columns=[\"TargetValues\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the overall statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train_data.describe()\n",
    "train_stats.pop(\"TargetValues\")\n",
    "train_stats = train_stats.transpose()\n",
    "print(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split features from labels\n",
    "- Separate the target value, or `label`, from the features.\n",
    "- This `label` is the value that you will train the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_data.pop('TargetValues')\n",
    "valid_labels = valid_data.pop('TargetValues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Normailized the Data</font>\n",
    "\n",
    "- It is good practice to normalize features that use different scales and ranges. \n",
    "- Although the model might converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x):\n",
    "  return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "# This normalized data is what we will use to train the model.\n",
    "normed_train_data = normalize_data(train_data)\n",
    "normed_valid_data = normalize_data(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Build the Model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate a sequential model using `keras`\n",
    "- `keras` is TensorFlow's high-level API for building and training deep learning models. It's used for fast prototyping, state-of-the-art research, and production.\n",
    "- The sequential model is the simplest model to use, especially when getting started.\n",
    "- It involves defining a Sequential class and adding layers to the model one by one in a linear manner, from input to output.\n",
    "- The model needs to know what input shape (`input_shape`) it should expect. The first layer of the `Sequential` model needs to receive the information.\n",
    "\n",
    "In the model below:\n",
    "\n",
    "- The model expects rows of data with num_shape variables (the input_shape=num_shape argument)\n",
    "- The first hidden layer has 64 nodes and uses the `relu` activation function.\n",
    "- The second hidden layer has 64 nodes and uses the `relu` activation function.\n",
    "- The output layer has one node and uses no activation function.\n",
    "\n",
    "The rectified linear activation function (`relu`) is a piecewise linear function that will output the input directly if is positive, otherwise, it will output zero. \n",
    "- Because rectified linear units are nearly linear, they preserve many of the properties that make linear models easy to optimize with gradient-based methods. They also preserve many of the properties that make linear models generalize well.\n",
    "- It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shape = len(train_data.keys())\n",
    "num_nodes = 64\n",
    "\n",
    "model = keras.Sequential([\n",
    "             layers.Dense(num_nodes, activation=tf.nn.relu, input_shape=[num_shape]),\n",
    "             layers.Dense(num_nodes, activation=tf.nn.relu),\n",
    "             layers.Dense(1) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model creation can also be written as:\n",
    "\n",
    "```python\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(num_nodes, activation=tf.nn.relu, input_shape=[num_shape]))\n",
    "model.add(layers.Dense(num_nodes, activation=tf.nn.relu))\n",
    "model.add(layers.Dense(1))\n",
    "```\n",
    "\n",
    "Dense layers represent a function that maps the input tensor `x` to an output tensor `y` via the equation `y = Ax + b` where `A` (the kernel) and `b` (the bias) are parameters of the dense layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model\n",
    "- Once you have specified the architecture of the network, you need to specify the method for back-propagation by choosing an optimizer and specify the loss.\n",
    "- Compiling the model uses the efficient numerical libraries (Theano or TensorFlow) in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.RMSprop(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required to provide a loss function and an optimizer: \n",
    "- We are asking the network to use the `rmsprop` optimizer to change weights in such a way that the loss `mse` (mean squared error) is minimized at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mse',\n",
    "              optimizer = optimizer,\n",
    "              metrics = ['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the model\n",
    "\n",
    "`model.summary()` is a useful method if you want to get an overview of your model and see the total number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the model\n",
    "\n",
    "10 samples from the training data and call `model.predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch = normed_train_data[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "print(example_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be working, and it produces a result of the expected shape and type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Train the Model</font>\n",
    "\n",
    "Training occurs over epochs and each epoch is split into batches.\n",
    "\n",
    "- **Epoch**: One pass through all of the rows in the training dataset.\n",
    "- **Batch**: One or more samples considered by the model within an epoch before weights are updated.\n",
    "- One epoch is comprised of one or more batches, based on the chosen batch size and the model is fit for many epochs. \n",
    "- The model is \"fit\" to the training data using the `fit` method. We also specify the `batch_size` and the maximum number of `epochs` we want training to go on.\n",
    "- The callback function is applied at given stages of the training procedure. We use it to get a view on internal states and statistics of the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 1000 epochs, and record the training and \n",
    "# validation accuracy in the history object\n",
    "\n",
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "      def on_epoch_end(self, epoch, logs):\n",
    "          if epoch % 100 == 0: \n",
    "             print('')\n",
    "          print('.', end='')\n",
    "\n",
    "# How many times we go through the entire dataset\n",
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(normed_train_data, train_labels,    \n",
    "                    epochs=EPOCHS, verbose=1, callbacks=[PrintDot()])\n",
    "#epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[PrintDot()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the model's training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the stats stored in the history object.\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Target]')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "             label='Train Error')\n",
    "#    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "#             label = 'Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([min(hist['mean_absolute_error']) ,max(hist['mean_absolute_error'])])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$Target^2$]')\n",
    "    plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "             label='Train Error')\n",
    "#    plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "#             label = 'Val Error')\n",
    "    plt.legend()\n",
    "    plt.ylim([0,max(hist['mean_squared_error'])])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Evaluate the Model on Test Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(normed_valid_data, valid_labels, verbose=1)\n",
    "#print(\"Testing set Mean Abs Error: {} \".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_predictions = model.predict(normed_valid_data).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the 45-degree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(valid_labels, valid_predictions);\n",
    "plt.xlabel('True Values');\n",
    "plt.ylabel('Predictions');\n",
    "plt.axis('equal');\n",
    "plt.axis('square');\n",
    "plt.xlim([0,plt.xlim()[1]]);\n",
    "plt.ylim([0,plt.ylim()[1]]);\n",
    "_ = plt.plot([-100, 100], [-100, 100]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(valid_predictions - valid_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Function Using Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure().gca(projection='3d');\n",
    "threedee.scatter(valid_data['x0'], valid_data['x1'], valid_predictions);\n",
    "threedee.set_xlabel('x');\n",
    "threedee.set_ylabel('y');\n",
    "threedee.set_zlabel('f(x,y)');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Exercise</font>\n",
    "\n",
    "Consider the 2D problem presented here.\n",
    "- Create a dataset of 1000 randomly selected points (in the domain) and their associated targets.\n",
    "- Randomly choose 80% of the data for training and the remaining for testing\n",
    "- Create your ML model and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
